{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モジュール・データ読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 314\n",
    "datasrc = \"data/official/\"\n",
    "train = pd.read_csv(os.path.join(datasrc, \"train.csv\"), index_col=0)\n",
    "x_train = train.drop(columns=[\"health\"])\n",
    "y_train = train[\"health\"]\n",
    "x_test = pd.read_csv(os.path.join(datasrc, \"test.csv\"), index_col=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 大文字区切りで単語を分割する関数を定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SplitThisStringByUppercase\n",
      "['Split', 'This', 'String', 'By', 'Uppercase']\n"
     ]
    }
   ],
   "source": [
    "def split_string_by_uppercase(input_string):\n",
    "    result = []\n",
    "    current_word = \"\"\n",
    "    if not type(input_string)==str:\n",
    "        return result\n",
    "\n",
    "    # Escape String \"Other\"\n",
    "    for char in input_string:\n",
    "        if char.isupper() and current_word:\n",
    "            result.append(current_word)\n",
    "            current_word = char\n",
    "        else:\n",
    "            current_word += char\n",
    "\n",
    "    if current_word:\n",
    "        result.append(current_word)\n",
    "\n",
    "    return result\n",
    "\n",
    "# 使用例\n",
    "input_string = \"SplitThisStringByUppercase\"\n",
    "print(input_string)\n",
    "result = split_string_by_uppercase(input_string)\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テストデータ固有の問題がないかを確認する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(train.problems.unique()) - set(x_test.problems.unique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 大文字区切りで単語を分割する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[:,\"problems_list\"] = train.problems.apply(split_string_by_uppercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Branch',\n",
       " 'Grates',\n",
       " 'Lights',\n",
       " 'Metal',\n",
       " 'Other',\n",
       " 'Root',\n",
       " 'Rope',\n",
       " 'Sneakers',\n",
       " 'Stones',\n",
       " 'Trunk',\n",
       " 'Wires'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problem_unique = set()\n",
    "for problems in train.loc[:,\"problems\"].unique():\n",
    "    for problem in split_string_by_uppercase(problems):\n",
    "        problem_unique.add(problem)\n",
    "problem_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>tree_dbh</th>\n",
       "      <th>curb_loc</th>\n",
       "      <th>health</th>\n",
       "      <th>steward</th>\n",
       "      <th>guards</th>\n",
       "      <th>sidewalk</th>\n",
       "      <th>user_type</th>\n",
       "      <th>problems</th>\n",
       "      <th>spc_common</th>\n",
       "      <th>...</th>\n",
       "      <th>is_problem_Other</th>\n",
       "      <th>is_problem_Trunk</th>\n",
       "      <th>is_problem_Metal</th>\n",
       "      <th>is_problem_Lights</th>\n",
       "      <th>is_problem_Wires</th>\n",
       "      <th>is_problem_Root</th>\n",
       "      <th>is_problem_Stones</th>\n",
       "      <th>is_problem_Branch</th>\n",
       "      <th>is_problem_Sneakers</th>\n",
       "      <th>is_problem_Rope</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-06-29</td>\n",
       "      <td>14</td>\n",
       "      <td>OnCurb</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Damage</td>\n",
       "      <td>Volunteer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>English oak</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-09-21</td>\n",
       "      <td>5</td>\n",
       "      <td>OnCurb</td>\n",
       "      <td>1</td>\n",
       "      <td>3or4</td>\n",
       "      <td>Helpful</td>\n",
       "      <td>NoDamage</td>\n",
       "      <td>Volunteer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>crimson king maple</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-09-13</td>\n",
       "      <td>26</td>\n",
       "      <td>OnCurb</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NoDamage</td>\n",
       "      <td>Volunteer</td>\n",
       "      <td>StonesBranchLights</td>\n",
       "      <td>English oak</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-05-09</td>\n",
       "      <td>15</td>\n",
       "      <td>OnCurb</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Damage</td>\n",
       "      <td>NYC Parks Staff</td>\n",
       "      <td>NaN</td>\n",
       "      <td>honeylocust</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-06-24</td>\n",
       "      <td>23</td>\n",
       "      <td>OnCurb</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NoDamage</td>\n",
       "      <td>Volunteer</td>\n",
       "      <td>Stones</td>\n",
       "      <td>London planetree</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19979</th>\n",
       "      <td>2016-07-15</td>\n",
       "      <td>19</td>\n",
       "      <td>OnCurb</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Damage</td>\n",
       "      <td>Volunteer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>shingle oak</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19980</th>\n",
       "      <td>2016-07-08</td>\n",
       "      <td>5</td>\n",
       "      <td>OnCurb</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NoDamage</td>\n",
       "      <td>NYC Parks Staff</td>\n",
       "      <td>NaN</td>\n",
       "      <td>catalpa</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19981</th>\n",
       "      <td>2015-08-20</td>\n",
       "      <td>21</td>\n",
       "      <td>OnCurb</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Damage</td>\n",
       "      <td>Volunteer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>English oak</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19982</th>\n",
       "      <td>2016-06-20</td>\n",
       "      <td>4</td>\n",
       "      <td>OnCurb</td>\n",
       "      <td>1</td>\n",
       "      <td>1or2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NoDamage</td>\n",
       "      <td>NYC Parks Staff</td>\n",
       "      <td>NaN</td>\n",
       "      <td>littleleaf linden</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19983</th>\n",
       "      <td>2015-08-19</td>\n",
       "      <td>31</td>\n",
       "      <td>OnCurb</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Damage</td>\n",
       "      <td>NYC Parks Staff</td>\n",
       "      <td>Stones</td>\n",
       "      <td>honeylocust</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19984 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       created_at  tree_dbh curb_loc  health steward   guards  sidewalk  \\\n",
       "0      2015-06-29        14   OnCurb       1     NaN      NaN    Damage   \n",
       "1      2016-09-21         5   OnCurb       1    3or4  Helpful  NoDamage   \n",
       "2      2015-09-13        26   OnCurb       2     NaN      NaN  NoDamage   \n",
       "3      2016-05-09        15   OnCurb       0     NaN      NaN    Damage   \n",
       "4      2016-06-24        23   OnCurb       1     NaN      NaN  NoDamage   \n",
       "...           ...       ...      ...     ...     ...      ...       ...   \n",
       "19979  2016-07-15        19   OnCurb       2     NaN      NaN    Damage   \n",
       "19980  2016-07-08         5   OnCurb       1     NaN      NaN  NoDamage   \n",
       "19981  2015-08-20        21   OnCurb       0     NaN      NaN    Damage   \n",
       "19982  2016-06-20         4   OnCurb       1    1or2      NaN  NoDamage   \n",
       "19983  2015-08-19        31   OnCurb       1     NaN      NaN    Damage   \n",
       "\n",
       "             user_type            problems          spc_common  ...  \\\n",
       "0            Volunteer                 NaN         English oak  ...   \n",
       "1            Volunteer                 NaN  crimson king maple  ...   \n",
       "2            Volunteer  StonesBranchLights         English oak  ...   \n",
       "3      NYC Parks Staff                 NaN         honeylocust  ...   \n",
       "4            Volunteer              Stones    London planetree  ...   \n",
       "...                ...                 ...                 ...  ...   \n",
       "19979        Volunteer                 NaN         shingle oak  ...   \n",
       "19980  NYC Parks Staff                 NaN             catalpa  ...   \n",
       "19981        Volunteer                 NaN         English oak  ...   \n",
       "19982  NYC Parks Staff                 NaN   littleleaf linden  ...   \n",
       "19983  NYC Parks Staff              Stones         honeylocust  ...   \n",
       "\n",
       "      is_problem_Other is_problem_Trunk is_problem_Metal  is_problem_Lights  \\\n",
       "0                False            False            False              False   \n",
       "1                False            False            False              False   \n",
       "2                False            False            False               True   \n",
       "3                False            False            False              False   \n",
       "4                False            False            False              False   \n",
       "...                ...              ...              ...                ...   \n",
       "19979            False            False            False              False   \n",
       "19980            False            False            False              False   \n",
       "19981            False            False            False              False   \n",
       "19982            False            False            False              False   \n",
       "19983            False            False            False              False   \n",
       "\n",
       "       is_problem_Wires is_problem_Root is_problem_Stones  is_problem_Branch  \\\n",
       "0                 False           False             False              False   \n",
       "1                 False           False             False              False   \n",
       "2                 False           False              True               True   \n",
       "3                 False           False             False              False   \n",
       "4                 False           False              True              False   \n",
       "...                 ...             ...               ...                ...   \n",
       "19979             False           False             False              False   \n",
       "19980             False           False             False              False   \n",
       "19981             False           False             False              False   \n",
       "19982             False           False             False              False   \n",
       "19983             False           False              True              False   \n",
       "\n",
       "       is_problem_Sneakers  is_problem_Rope  \n",
       "0                    False            False  \n",
       "1                    False            False  \n",
       "2                    False            False  \n",
       "3                    False            False  \n",
       "4                    False            False  \n",
       "...                    ...              ...  \n",
       "19979                False            False  \n",
       "19980                False            False  \n",
       "19981                False            False  \n",
       "19982                False            False  \n",
       "19983                False            False  \n",
       "\n",
       "[19984 rows x 33 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for unique_problem in problem_unique:\n",
    "    train.loc[:,f\"is_problem_{unique_problem}\"] = train.loc[:,\"problems_list\"].apply(lambda x: unique_problem in x)\n",
    "train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 問題の数を可視化\n",
    "→Otherも1単語としてカウントされている為、他の単語と比べて多くなっている問題がある"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     12243\n",
       "2      2312\n",
       "1      2219\n",
       "3      1393\n",
       "4       643\n",
       "5       467\n",
       "6       406\n",
       "7       180\n",
       "8        92\n",
       "11       15\n",
       "9        14\n",
       "Name: problems_count, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.loc[:,\"problems_count\"] = train.loc[:,\"problems_list\"].apply(len)\n",
    "train.loc[:,\"problems_count\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: >"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train.loc[:,\"problems_list\"].apply(len).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>problems_count</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>health</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.57</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "problems_count    0     1     2     3     4     5     6     7     8    9    11\n",
       "health                                                                        \n",
       "0               0.57  0.12  0.12  0.08  0.03  0.03  0.02  0.01  0.01  0.0  0.0\n",
       "1               0.62  0.11  0.12  0.07  0.03  0.02  0.02  0.01  0.00  0.0  0.0\n",
       "2               0.62  0.08  0.10  0.07  0.04  0.04  0.03  0.02  0.01  0.0  NaN"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.groupby(\"health\")[\"problems_count\"].value_counts(normalize=True,).round(2).unstack()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 木の部位ごとに問題があるのかの特徴量を作成"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特徴量の可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Branch</th>\n",
       "      <th>Trunk</th>\n",
       "      <th>Root</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>16123</td>\n",
       "      <td>18498</td>\n",
       "      <td>18573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>3861</td>\n",
       "      <td>1486</td>\n",
       "      <td>1411</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Branch  Trunk   Root\n",
       "False   16123  18498  18573\n",
       "True     3861   1486   1411"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parts_valuecounts = list()\n",
    "parts_list = [\n",
    "    \"Branch\",\n",
    "    \"Trunk\",\n",
    "    \"Root\",\n",
    "]\n",
    "for parts in parts_list:\n",
    "    parts_valuecounts.append(train.loc[:,f\"is_problem_{parts}\"].value_counts().round(2))\n",
    "pd.concat(parts_valuecounts, axis=1, keys=parts_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>is_problem_Branch</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>health</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.778784</td>\n",
       "      <td>0.221216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.813663</td>\n",
       "      <td>0.186337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.793696</td>\n",
       "      <td>0.206304</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "is_problem_Branch     False     True \n",
       "health                               \n",
       "0                  0.778784  0.221216\n",
       "1                  0.813663  0.186337\n",
       "2                  0.793696  0.206304"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.groupby(\"health\")[\"is_problem_Branch\"].value_counts(normalize=True).unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>is_problem_Trunk</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>health</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.923055</td>\n",
       "      <td>0.076945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.927433</td>\n",
       "      <td>0.072567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.898281</td>\n",
       "      <td>0.101719</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "is_problem_Trunk     False     True \n",
       "health                              \n",
       "0                 0.923055  0.076945\n",
       "1                 0.927433  0.072567\n",
       "2                 0.898281  0.101719"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.groupby(\"health\")[\"is_problem_Trunk\"].value_counts(normalize=True).unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>is_problem_Root</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>health</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.918529</td>\n",
       "      <td>0.081471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.932893</td>\n",
       "      <td>0.067107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.905444</td>\n",
       "      <td>0.094556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "is_problem_Root     False     True \n",
       "health                             \n",
       "0                0.918529  0.081471\n",
       "1                0.932893  0.067107\n",
       "2                0.905444  0.094556"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.groupby(\"health\")[\"is_problem_Root\"].value_counts(normalize=True).unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>health</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_problem_Branch</th>\n",
       "      <th>is_problem_Trunk</th>\n",
       "      <th>is_problem_Root</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">False</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">False</th>\n",
       "      <th>False</th>\n",
       "      <td>0.71</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">True</th>\n",
       "      <th>False</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">True</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">False</th>\n",
       "      <th>False</th>\n",
       "      <td>0.16</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">True</th>\n",
       "      <th>False</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "health                                                 0     1     2\n",
       "is_problem_Branch is_problem_Trunk is_problem_Root                  \n",
       "False             False            False            0.71  0.75  0.72\n",
       "                                   True             0.03  0.03  0.03\n",
       "                  True             False            0.02  0.03  0.03\n",
       "                                   True             0.01  0.01  0.02\n",
       "True              False            False            0.16  0.14  0.13\n",
       "                                   True             0.02  0.01  0.03\n",
       "                  True             False            0.02  0.02  0.03\n",
       "                                   True             0.02  0.02  0.02"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.groupby(\"health\")[[f\"is_problem_{parts}\" for parts in parts_list]].value_counts(normalize=True,).round(2).unstack(level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all                1         0         2\n",
      "health  0.788181  0.176892  0.034928\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>health</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_problem_Branch</th>\n",
       "      <th>is_problem_Trunk</th>\n",
       "      <th>is_problem_Root</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">False</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">False</th>\n",
       "      <th>False</th>\n",
       "      <td>0.17</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">True</th>\n",
       "      <th>False</th>\n",
       "      <td>0.17</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">True</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">False</th>\n",
       "      <th>False</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>0.22</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">True</th>\n",
       "      <th>False</th>\n",
       "      <td>0.18</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>0.19</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "health                                                 0     1     2\n",
       "is_problem_Branch is_problem_Trunk is_problem_Root                  \n",
       "False             False            False            0.17  0.80  0.03\n",
       "                                   True             0.20  0.77  0.04\n",
       "                  True             False            0.17  0.80  0.04\n",
       "                                   True             0.21  0.72  0.07\n",
       "True              False            False            0.21  0.76  0.03\n",
       "                                   True             0.22  0.72  0.06\n",
       "                  True             False            0.18  0.76  0.06\n",
       "                                   True             0.19  0.77  0.04"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"all\",pd.DataFrame(train.health.value_counts(normalize=True), ).T)\n",
    "train.groupby(\"health\")[[f\"is_problem_{parts}\" for parts in parts_list]].value_counts().unstack(level=0).apply(lambda x: x/x.sum(), axis=1).round(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "木の部位の組み合わせや数について、ラベルを比較しても割合については差がない  \n",
    "つまり、木の部位の組み合わせや数は問題の特徴量として有効でない可能性が高い。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特徴量の可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Grates</th>\n",
       "      <th>Lights</th>\n",
       "      <th>Metal</th>\n",
       "      <th>Other</th>\n",
       "      <th>Rope</th>\n",
       "      <th>Sneakers</th>\n",
       "      <th>Stones</th>\n",
       "      <th>Wires</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>19679</td>\n",
       "      <td>17268</td>\n",
       "      <td>19679</td>\n",
       "      <td>17026</td>\n",
       "      <td>19092</td>\n",
       "      <td>19974</td>\n",
       "      <td>15686</td>\n",
       "      <td>19092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>305</td>\n",
       "      <td>2716</td>\n",
       "      <td>305</td>\n",
       "      <td>2958</td>\n",
       "      <td>892</td>\n",
       "      <td>10</td>\n",
       "      <td>4298</td>\n",
       "      <td>892</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Grates  Lights  Metal  Other   Rope  Sneakers  Stones  Wires\n",
       "False   19679   17268  19679  17026  19092     19974   15686  19092\n",
       "True      305    2716    305   2958    892        10    4298    892"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cause_valuecounts = list()\n",
    "cause_list = [\n",
    "        'Grates',\n",
    "        'Lights',\n",
    "        'Metal',\n",
    "        'Other',\n",
    "        'Rope',\n",
    "        'Sneakers',\n",
    "        'Stones',\n",
    "        'Wires'\n",
    "        ]\n",
    "for cause in cause_list:\n",
    "    cause_valuecounts.append(train.loc[:,f\"is_problem_{cause}\"].value_counts())\n",
    "pd.DataFrame(cause_valuecounts, index=cause_list).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>is_problem_Grates</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>health</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.985007</td>\n",
       "      <td>0.014993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.985080</td>\n",
       "      <td>0.014920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.975645</td>\n",
       "      <td>0.024355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "is_problem_Grates     False     True \n",
       "health                               \n",
       "0                  0.985007  0.014993\n",
       "1                  0.985080  0.014920\n",
       "2                  0.975645  0.024355"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.groupby(\"health\")[\"is_problem_Grates\"].value_counts(normalize=True).unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>is_problem_Lights</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>health</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.839038</td>\n",
       "      <td>0.160962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.870484</td>\n",
       "      <td>0.129516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.846705</td>\n",
       "      <td>0.153295</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "is_problem_Lights     False     True \n",
       "health                               \n",
       "0                  0.839038  0.160962\n",
       "1                  0.870484  0.129516\n",
       "2                  0.846705  0.153295"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.groupby(\"health\")[\"is_problem_Lights\"].value_counts(normalize=True).unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>is_problem_Metal</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>health</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.985007</td>\n",
       "      <td>0.014993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.985080</td>\n",
       "      <td>0.014920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.975645</td>\n",
       "      <td>0.024355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "is_problem_Metal     False     True \n",
       "health                              \n",
       "0                 0.985007  0.014993\n",
       "1                 0.985080  0.014920\n",
       "2                 0.975645  0.024355"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.groupby(\"health\")[\"is_problem_Metal\"].value_counts(normalize=True).unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>is_problem_Other</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>health</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.839604</td>\n",
       "      <td>0.160396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.855374</td>\n",
       "      <td>0.144626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.838109</td>\n",
       "      <td>0.161891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "is_problem_Other     False     True \n",
       "health                              \n",
       "0                 0.839604  0.160396\n",
       "1                 0.855374  0.144626\n",
       "2                 0.838109  0.161891"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.groupby(\"health\")[\"is_problem_Other\"].value_counts(normalize=True).unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>is_problem_Rope</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>health</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.950495</td>\n",
       "      <td>0.049505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.957780</td>\n",
       "      <td>0.042220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.925501</td>\n",
       "      <td>0.074499</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "is_problem_Rope     False     True \n",
       "health                             \n",
       "0                0.950495  0.049505\n",
       "1                0.957780  0.042220\n",
       "2                0.925501  0.074499"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.groupby(\"health\")[\"is_problem_Rope\"].value_counts(normalize=True).unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>is_problem_Sneakers</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>health</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.999434</td>\n",
       "      <td>0.000566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.999492</td>\n",
       "      <td>0.000508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "is_problem_Sneakers     False     True \n",
       "health                                 \n",
       "0                    0.999434  0.000566\n",
       "1                    0.999492  0.000508\n",
       "2                    1.000000       NaN"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.groupby(\"health\")[\"is_problem_Sneakers\"].value_counts(normalize=True).unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>is_problem_Stones</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>health</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.755304</td>\n",
       "      <td>0.244696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.791251</td>\n",
       "      <td>0.208749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.792264</td>\n",
       "      <td>0.207736</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "is_problem_Stones     False     True \n",
       "health                               \n",
       "0                  0.755304  0.244696\n",
       "1                  0.791251  0.208749\n",
       "2                  0.792264  0.207736"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.groupby(\"health\")[\"is_problem_Stones\"].value_counts(normalize=True).unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>is_problem_Wires</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>health</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.950495</td>\n",
       "      <td>0.049505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.957780</td>\n",
       "      <td>0.042220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.925501</td>\n",
       "      <td>0.074499</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "is_problem_Wires     False     True \n",
       "health                              \n",
       "0                 0.950495  0.049505\n",
       "1                 0.957780  0.042220\n",
       "2                 0.925501  0.074499"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.groupby(\"health\")[\"is_problem_Wires\"].value_counts(normalize=True).unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>health</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_problem_Grates</th>\n",
       "      <th>is_problem_Lights</th>\n",
       "      <th>is_problem_Metal</th>\n",
       "      <th>is_problem_Other</th>\n",
       "      <th>is_problem_Rope</th>\n",
       "      <th>is_problem_Sneakers</th>\n",
       "      <th>is_problem_Stones</th>\n",
       "      <th>is_problem_Wires</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"17\" valign=\"top\">False</th>\n",
       "      <th rowspan=\"9\" valign=\"top\">False</th>\n",
       "      <th rowspan=\"9\" valign=\"top\">False</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">False</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">False</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">False</th>\n",
       "      <th>False</th>\n",
       "      <th>False</th>\n",
       "      <td>0.16</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th>False</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th>True</th>\n",
       "      <th>False</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.80</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">True</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">False</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "      <td>0.14</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th>True</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">True</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">False</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">False</th>\n",
       "      <th>False</th>\n",
       "      <th>False</th>\n",
       "      <td>0.18</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th>False</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">True</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">False</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "      <td>0.17</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th>True</th>\n",
       "      <td>0.16</td>\n",
       "      <td>0.84</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">True</th>\n",
       "      <th rowspan=\"8\" valign=\"top\">False</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">False</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">False</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">False</th>\n",
       "      <th>False</th>\n",
       "      <th>False</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th>False</th>\n",
       "      <td>0.22</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">True</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">False</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th>True</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">True</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">False</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">False</th>\n",
       "      <th>False</th>\n",
       "      <th>False</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th>False</th>\n",
       "      <td>0.17</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">True</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">False</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "      <td>0.26</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th>True</th>\n",
       "      <td>0.26</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">True</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">False</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">True</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">False</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">False</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">False</th>\n",
       "      <th>False</th>\n",
       "      <th>False</th>\n",
       "      <td>0.12</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th>False</th>\n",
       "      <td>0.12</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th>False</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "      <td>0.27</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">True</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">False</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">False</th>\n",
       "      <th>False</th>\n",
       "      <th>False</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th>False</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">True</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">True</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">False</th>\n",
       "      <th>False</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "      <th>False</th>\n",
       "      <td>0.22</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "      <th>True</th>\n",
       "      <td>0.14</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "health                                                                                                                                           0  \\\n",
       "is_problem_Grates is_problem_Lights is_problem_Metal is_problem_Other is_problem_Rope is_problem_Sneakers is_problem_Stones is_problem_Wires         \n",
       "False             False             False            False            False           False               False             False             0.16   \n",
       "                                                                                                          True              False             0.20   \n",
       "                                                                                      True                True              False             0.20   \n",
       "                                                                      True            False               False             True              0.14   \n",
       "                                                                                                          True              True              0.10   \n",
       "                                                     True             False           False               False             False             0.18   \n",
       "                                                                                                          True              False             0.20   \n",
       "                                                                      True            False               False             True              0.17   \n",
       "                                                                                                          True              True              0.16   \n",
       "                  True              False            False            False           False               False             False             0.20   \n",
       "                                                                                                          True              False             0.22   \n",
       "                                                                      True            False               False             True              0.25   \n",
       "                                                                                                          True              True              0.20   \n",
       "                                                     True             False           False               False             False             0.20   \n",
       "                                                                                                          True              False             0.17   \n",
       "                                                                      True            False               False             True              0.26   \n",
       "                                                                                                          True              True              0.26   \n",
       "True              False             True             False            False           False               False             False             0.12   \n",
       "                                                                                                          True              False             0.12   \n",
       "                                                                      True            False               False             True              0.27   \n",
       "                                                     True             False           False               False             False             0.25   \n",
       "                                                                                                          True              False              NaN   \n",
       "                  True              True             False            False           False               True              False             0.22   \n",
       "                                                                      True            False               True              True              0.14   \n",
       "\n",
       "health                                                                                                                                           1  \\\n",
       "is_problem_Grates is_problem_Lights is_problem_Metal is_problem_Other is_problem_Rope is_problem_Sneakers is_problem_Stones is_problem_Wires         \n",
       "False             False             False            False            False           False               False             False             0.80   \n",
       "                                                                                                          True              False             0.78   \n",
       "                                                                                      True                True              False             0.80   \n",
       "                                                                      True            False               False             True              0.82   \n",
       "                                                                                                          True              True              0.84   \n",
       "                                                     True             False           False               False             False             0.79   \n",
       "                                                                                                          True              False             0.77   \n",
       "                                                                      True            False               False             True              0.75   \n",
       "                                                                                                          True              True              0.84   \n",
       "                  True              False            False            False           False               False             False             0.77   \n",
       "                                                                                                          True              False             0.74   \n",
       "                                                                      True            False               False             True              0.73   \n",
       "                                                                                                          True              True              0.72   \n",
       "                                                     True             False           False               False             False             0.77   \n",
       "                                                                                                          True              False             0.77   \n",
       "                                                                      True            False               False             True              0.66   \n",
       "                                                                                                          True              True              0.70   \n",
       "True              False             True             False            False           False               False             False             0.86   \n",
       "                                                                                                          True              False             0.82   \n",
       "                                                                      True            False               False             True              0.55   \n",
       "                                                     True             False           False               False             False             0.70   \n",
       "                                                                                                          True              False             0.50   \n",
       "                  True              True             False            False           False               True              False             0.72   \n",
       "                                                                      True            False               True              True              0.71   \n",
       "\n",
       "health                                                                                                                                           2  \n",
       "is_problem_Grates is_problem_Lights is_problem_Metal is_problem_Other is_problem_Rope is_problem_Sneakers is_problem_Stones is_problem_Wires        \n",
       "False             False             False            False            False           False               False             False             0.04  \n",
       "                                                                                                          True              False             0.02  \n",
       "                                                                                      True                True              False              NaN  \n",
       "                                                                      True            False               False             True              0.04  \n",
       "                                                                                                          True              True              0.06  \n",
       "                                                     True             False           False               False             False             0.03  \n",
       "                                                                                                          True              False             0.04  \n",
       "                                                                      True            False               False             True              0.08  \n",
       "                                                                                                          True              True               NaN  \n",
       "                  True              False            False            False           False               False             False             0.04  \n",
       "                                                                                                          True              False             0.04  \n",
       "                                                                      True            False               False             True              0.02  \n",
       "                                                                                                          True              True              0.07  \n",
       "                                                     True             False           False               False             False             0.02  \n",
       "                                                                                                          True              False             0.06  \n",
       "                                                                      True            False               False             True              0.08  \n",
       "                                                                                                          True              True              0.04  \n",
       "True              False             True             False            False           False               False             False             0.03  \n",
       "                                                                                                          True              False             0.06  \n",
       "                                                                      True            False               False             True              0.18  \n",
       "                                                     True             False           False               False             False             0.05  \n",
       "                                                                                                          True              False             0.50  \n",
       "                  True              True             False            False           False               True              False             0.06  \n",
       "                                                                      True            False               True              True              0.14  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.groupby(\"health\")[[f\"is_problem_{cause}\" for cause in cause_list]].value_counts().unstack(level=0).apply(lambda x: x/x.sum(), axis=1).round(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cols = [\n",
    "    \"problems_count\",\n",
    "    *[f\"is_problem_{parts}\" for parts in parts_list],\n",
    "    *[f\"is_problem_{cause}\" for cause in cause_list],\n",
    "]\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(train.loc[:,use_cols], train[\"health\"], test_size=0.2, random_state=314, stratify=train[\"health\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, recall_score\n",
    "\n",
    "def mean_recall(preds:np.ndarray,eval_data: lgb.Dataset):\n",
    "    y_true = eval_data.get_label()\n",
    "    weight = eval_data.get_weight()\n",
    "    preds = preds.reshape(len(np.unique(y_true)), -1)\n",
    "    preds = preds.argmax(axis = 0)\n",
    "    recall = recall_score(y_true,preds,average='macro',sample_weight=weight)\n",
    "    return 'recall',recall,True\n",
    "\n",
    "def mean_f1score(preds:np.ndarray,eval_data: lgb.Dataset):\n",
    "    y_true = eval_data.get_label()\n",
    "    weight = eval_data.get_weight()\n",
    "    preds = preds.reshape(len(np.unique(y_true)), -1)\n",
    "    preds = preds.argmax(axis = 0)\n",
    "    f1 = f1_score(y_true,preds,average='macro',sample_weight=weight)\n",
    "    return 'f1',f1,True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002111 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 31\n",
      "[LightGBM] [Info] Number of data points in the train set: 15987, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score -1.732206\n",
      "[LightGBM] [Info] Start training from score -0.238000\n",
      "[LightGBM] [Info] Start training from score -3.355172\n",
      "[1]\ttraining's multi_logloss: 0.609901\ttraining's f1: 0.293853\n",
      "[2]\ttraining's multi_logloss: 0.609015\ttraining's f1: 0.293853\n",
      "[3]\ttraining's multi_logloss: 0.608322\ttraining's f1: 0.293853\n",
      "[4]\ttraining's multi_logloss: 0.607778\ttraining's f1: 0.293853\n",
      "[5]\ttraining's multi_logloss: 0.607311\ttraining's f1: 0.293853\n",
      "[6]\ttraining's multi_logloss: 0.606939\ttraining's f1: 0.293853\n",
      "[7]\ttraining's multi_logloss: 0.606623\ttraining's f1: 0.293853\n",
      "[8]\ttraining's multi_logloss: 0.606356\ttraining's f1: 0.293853\n",
      "[9]\ttraining's multi_logloss: 0.60612\ttraining's f1: 0.293853\n",
      "[10]\ttraining's multi_logloss: 0.605911\ttraining's f1: 0.293853\n",
      "[11]\ttraining's multi_logloss: 0.605747\ttraining's f1: 0.293853\n",
      "[12]\ttraining's multi_logloss: 0.605593\ttraining's f1: 0.293853\n",
      "[13]\ttraining's multi_logloss: 0.605463\ttraining's f1: 0.293853\n",
      "[14]\ttraining's multi_logloss: 0.605351\ttraining's f1: 0.293853\n",
      "[15]\ttraining's multi_logloss: 0.605255\ttraining's f1: 0.293853\n",
      "[16]\ttraining's multi_logloss: 0.60516\ttraining's f1: 0.293853\n",
      "[17]\ttraining's multi_logloss: 0.605081\ttraining's f1: 0.293853\n",
      "[18]\ttraining's multi_logloss: 0.605009\ttraining's f1: 0.293853\n",
      "[19]\ttraining's multi_logloss: 0.604943\ttraining's f1: 0.293853\n",
      "[20]\ttraining's multi_logloss: 0.604879\ttraining's f1: 0.293853\n",
      "[21]\ttraining's multi_logloss: 0.604826\ttraining's f1: 0.293853\n",
      "[22]\ttraining's multi_logloss: 0.604773\ttraining's f1: 0.293853\n",
      "[23]\ttraining's multi_logloss: 0.604724\ttraining's f1: 0.293853\n",
      "[24]\ttraining's multi_logloss: 0.604681\ttraining's f1: 0.293853\n",
      "[25]\ttraining's multi_logloss: 0.60462\ttraining's f1: 0.293853\n",
      "[26]\ttraining's multi_logloss: 0.604569\ttraining's f1: 0.293853\n",
      "[27]\ttraining's multi_logloss: 0.60453\ttraining's f1: 0.293853\n",
      "[28]\ttraining's multi_logloss: 0.604489\ttraining's f1: 0.293853\n",
      "[29]\ttraining's multi_logloss: 0.604462\ttraining's f1: 0.293853\n",
      "[30]\ttraining's multi_logloss: 0.604437\ttraining's f1: 0.293853\n",
      "[31]\ttraining's multi_logloss: 0.604411\ttraining's f1: 0.293853\n",
      "[32]\ttraining's multi_logloss: 0.60438\ttraining's f1: 0.293853\n",
      "[33]\ttraining's multi_logloss: 0.604355\ttraining's f1: 0.293853\n",
      "[34]\ttraining's multi_logloss: 0.604336\ttraining's f1: 0.293853\n",
      "[35]\ttraining's multi_logloss: 0.604319\ttraining's f1: 0.293853\n",
      "[36]\ttraining's multi_logloss: 0.604288\ttraining's f1: 0.293853\n",
      "[37]\ttraining's multi_logloss: 0.604273\ttraining's f1: 0.293853\n",
      "[38]\ttraining's multi_logloss: 0.604253\ttraining's f1: 0.293853\n",
      "[39]\ttraining's multi_logloss: 0.604237\ttraining's f1: 0.293853\n",
      "[40]\ttraining's multi_logloss: 0.604223\ttraining's f1: 0.293853\n",
      "[41]\ttraining's multi_logloss: 0.604209\ttraining's f1: 0.293853\n",
      "[42]\ttraining's multi_logloss: 0.604199\ttraining's f1: 0.293853\n",
      "[43]\ttraining's multi_logloss: 0.604186\ttraining's f1: 0.293853\n",
      "[44]\ttraining's multi_logloss: 0.604171\ttraining's f1: 0.293853\n",
      "[45]\ttraining's multi_logloss: 0.604157\ttraining's f1: 0.293853\n",
      "[46]\ttraining's multi_logloss: 0.60415\ttraining's f1: 0.293853\n",
      "[47]\ttraining's multi_logloss: 0.604135\ttraining's f1: 0.293853\n",
      "[48]\ttraining's multi_logloss: 0.604121\ttraining's f1: 0.293853\n",
      "[49]\ttraining's multi_logloss: 0.604109\ttraining's f1: 0.296237\n",
      "[50]\ttraining's multi_logloss: 0.604099\ttraining's f1: 0.296237\n",
      "[51]\ttraining's multi_logloss: 0.60409\ttraining's f1: 0.296237\n",
      "[52]\ttraining's multi_logloss: 0.604082\ttraining's f1: 0.296237\n",
      "[53]\ttraining's multi_logloss: 0.604072\ttraining's f1: 0.296237\n",
      "[54]\ttraining's multi_logloss: 0.604065\ttraining's f1: 0.296237\n",
      "[55]\ttraining's multi_logloss: 0.604059\ttraining's f1: 0.296237\n",
      "[56]\ttraining's multi_logloss: 0.604052\ttraining's f1: 0.296237\n",
      "[57]\ttraining's multi_logloss: 0.604044\ttraining's f1: 0.296237\n",
      "[58]\ttraining's multi_logloss: 0.604037\ttraining's f1: 0.296237\n",
      "[59]\ttraining's multi_logloss: 0.604029\ttraining's f1: 0.296237\n",
      "[60]\ttraining's multi_logloss: 0.604023\ttraining's f1: 0.296237\n",
      "[61]\ttraining's multi_logloss: 0.604012\ttraining's f1: 0.296237\n",
      "[62]\ttraining's multi_logloss: 0.604007\ttraining's f1: 0.296237\n",
      "[63]\ttraining's multi_logloss: 0.603998\ttraining's f1: 0.296237\n",
      "[64]\ttraining's multi_logloss: 0.603992\ttraining's f1: 0.296237\n",
      "[65]\ttraining's multi_logloss: 0.603986\ttraining's f1: 0.296237\n",
      "[66]\ttraining's multi_logloss: 0.603982\ttraining's f1: 0.296237\n",
      "[67]\ttraining's multi_logloss: 0.603978\ttraining's f1: 0.296237\n",
      "[68]\ttraining's multi_logloss: 0.603973\ttraining's f1: 0.296237\n",
      "[69]\ttraining's multi_logloss: 0.60397\ttraining's f1: 0.296237\n",
      "[70]\ttraining's multi_logloss: 0.603965\ttraining's f1: 0.296237\n",
      "[71]\ttraining's multi_logloss: 0.60396\ttraining's f1: 0.296237\n",
      "[72]\ttraining's multi_logloss: 0.603956\ttraining's f1: 0.296237\n",
      "[73]\ttraining's multi_logloss: 0.603953\ttraining's f1: 0.296237\n",
      "[74]\ttraining's multi_logloss: 0.603951\ttraining's f1: 0.296237\n",
      "[75]\ttraining's multi_logloss: 0.603946\ttraining's f1: 0.296237\n",
      "[76]\ttraining's multi_logloss: 0.603943\ttraining's f1: 0.296237\n",
      "[77]\ttraining's multi_logloss: 0.60394\ttraining's f1: 0.296237\n",
      "[78]\ttraining's multi_logloss: 0.603936\ttraining's f1: 0.296237\n",
      "[79]\ttraining's multi_logloss: 0.603932\ttraining's f1: 0.296237\n",
      "[80]\ttraining's multi_logloss: 0.60393\ttraining's f1: 0.296237\n",
      "[81]\ttraining's multi_logloss: 0.603927\ttraining's f1: 0.296237\n",
      "[82]\ttraining's multi_logloss: 0.603925\ttraining's f1: 0.296237\n",
      "[83]\ttraining's multi_logloss: 0.603922\ttraining's f1: 0.296237\n",
      "[84]\ttraining's multi_logloss: 0.60392\ttraining's f1: 0.296237\n",
      "[85]\ttraining's multi_logloss: 0.603918\ttraining's f1: 0.296237\n",
      "[86]\ttraining's multi_logloss: 0.603915\ttraining's f1: 0.296237\n",
      "[87]\ttraining's multi_logloss: 0.603913\ttraining's f1: 0.296237\n",
      "[88]\ttraining's multi_logloss: 0.603911\ttraining's f1: 0.296237\n",
      "[89]\ttraining's multi_logloss: 0.603909\ttraining's f1: 0.296237\n",
      "[90]\ttraining's multi_logloss: 0.603908\ttraining's f1: 0.296237\n",
      "[91]\ttraining's multi_logloss: 0.603905\ttraining's f1: 0.296237\n",
      "[92]\ttraining's multi_logloss: 0.603904\ttraining's f1: 0.296237\n",
      "[93]\ttraining's multi_logloss: 0.603901\ttraining's f1: 0.296237\n",
      "[94]\ttraining's multi_logloss: 0.603899\ttraining's f1: 0.296237\n",
      "[95]\ttraining's multi_logloss: 0.603897\ttraining's f1: 0.296237\n",
      "[96]\ttraining's multi_logloss: 0.603894\ttraining's f1: 0.296237\n",
      "[97]\ttraining's multi_logloss: 0.603893\ttraining's f1: 0.296237\n",
      "[98]\ttraining's multi_logloss: 0.603891\ttraining's f1: 0.296237\n",
      "[99]\ttraining's multi_logloss: 0.603889\ttraining's f1: 0.296237\n",
      "[100]\ttraining's multi_logloss: 0.603888\ttraining's f1: 0.296237\n",
      "[101]\ttraining's multi_logloss: 0.603886\ttraining's f1: 0.296237\n",
      "[102]\ttraining's multi_logloss: 0.603884\ttraining's f1: 0.296237\n",
      "[103]\ttraining's multi_logloss: 0.603883\ttraining's f1: 0.296237\n",
      "[104]\ttraining's multi_logloss: 0.603881\ttraining's f1: 0.296237\n",
      "[105]\ttraining's multi_logloss: 0.603879\ttraining's f1: 0.296237\n",
      "[106]\ttraining's multi_logloss: 0.603878\ttraining's f1: 0.296237\n",
      "[107]\ttraining's multi_logloss: 0.603876\ttraining's f1: 0.296237\n",
      "[108]\ttraining's multi_logloss: 0.603874\ttraining's f1: 0.296237\n",
      "[109]\ttraining's multi_logloss: 0.603871\ttraining's f1: 0.296237\n",
      "[110]\ttraining's multi_logloss: 0.60387\ttraining's f1: 0.296237\n",
      "[111]\ttraining's multi_logloss: 0.603868\ttraining's f1: 0.296237\n",
      "[112]\ttraining's multi_logloss: 0.603867\ttraining's f1: 0.296237\n",
      "[113]\ttraining's multi_logloss: 0.603865\ttraining's f1: 0.296237\n",
      "[114]\ttraining's multi_logloss: 0.603864\ttraining's f1: 0.296237\n",
      "[115]\ttraining's multi_logloss: 0.603862\ttraining's f1: 0.296237\n",
      "[116]\ttraining's multi_logloss: 0.603861\ttraining's f1: 0.296237\n",
      "[117]\ttraining's multi_logloss: 0.60386\ttraining's f1: 0.296237\n",
      "[118]\ttraining's multi_logloss: 0.603858\ttraining's f1: 0.296237\n",
      "[119]\ttraining's multi_logloss: 0.603857\ttraining's f1: 0.296237\n",
      "[120]\ttraining's multi_logloss: 0.603856\ttraining's f1: 0.296237\n",
      "[121]\ttraining's multi_logloss: 0.603854\ttraining's f1: 0.296237\n",
      "[122]\ttraining's multi_logloss: 0.603853\ttraining's f1: 0.296237\n",
      "[123]\ttraining's multi_logloss: 0.603851\ttraining's f1: 0.296237\n",
      "[124]\ttraining's multi_logloss: 0.60385\ttraining's f1: 0.296237\n",
      "[125]\ttraining's multi_logloss: 0.603849\ttraining's f1: 0.296237\n",
      "[126]\ttraining's multi_logloss: 0.603848\ttraining's f1: 0.296237\n",
      "[127]\ttraining's multi_logloss: 0.603847\ttraining's f1: 0.296237\n",
      "[128]\ttraining's multi_logloss: 0.603846\ttraining's f1: 0.296237\n",
      "[129]\ttraining's multi_logloss: 0.603844\ttraining's f1: 0.296237\n",
      "[130]\ttraining's multi_logloss: 0.603843\ttraining's f1: 0.296237\n",
      "[131]\ttraining's multi_logloss: 0.603842\ttraining's f1: 0.296237\n",
      "[132]\ttraining's multi_logloss: 0.603841\ttraining's f1: 0.296237\n",
      "[133]\ttraining's multi_logloss: 0.60384\ttraining's f1: 0.296237\n",
      "[134]\ttraining's multi_logloss: 0.603839\ttraining's f1: 0.296237\n",
      "[135]\ttraining's multi_logloss: 0.603838\ttraining's f1: 0.296237\n",
      "[136]\ttraining's multi_logloss: 0.603837\ttraining's f1: 0.296237\n",
      "[137]\ttraining's multi_logloss: 0.603836\ttraining's f1: 0.296237\n",
      "[138]\ttraining's multi_logloss: 0.603835\ttraining's f1: 0.296237\n",
      "[139]\ttraining's multi_logloss: 0.603834\ttraining's f1: 0.296237\n",
      "[140]\ttraining's multi_logloss: 0.603833\ttraining's f1: 0.296237\n",
      "[141]\ttraining's multi_logloss: 0.603832\ttraining's f1: 0.296237\n",
      "[142]\ttraining's multi_logloss: 0.60383\ttraining's f1: 0.296237\n",
      "[143]\ttraining's multi_logloss: 0.603829\ttraining's f1: 0.296237\n",
      "[144]\ttraining's multi_logloss: 0.603828\ttraining's f1: 0.296237\n",
      "[145]\ttraining's multi_logloss: 0.603826\ttraining's f1: 0.296237\n",
      "[146]\ttraining's multi_logloss: 0.603825\ttraining's f1: 0.296237\n",
      "[147]\ttraining's multi_logloss: 0.603824\ttraining's f1: 0.296237\n",
      "[148]\ttraining's multi_logloss: 0.603824\ttraining's f1: 0.296237\n",
      "[149]\ttraining's multi_logloss: 0.603823\ttraining's f1: 0.296237\n",
      "[150]\ttraining's multi_logloss: 0.603822\ttraining's f1: 0.296237\n",
      "[151]\ttraining's multi_logloss: 0.603821\ttraining's f1: 0.296237\n",
      "[152]\ttraining's multi_logloss: 0.60382\ttraining's f1: 0.296237\n",
      "[153]\ttraining's multi_logloss: 0.60382\ttraining's f1: 0.296237\n",
      "[154]\ttraining's multi_logloss: 0.603818\ttraining's f1: 0.296237\n",
      "[155]\ttraining's multi_logloss: 0.603818\ttraining's f1: 0.296237\n",
      "[156]\ttraining's multi_logloss: 0.603817\ttraining's f1: 0.296237\n",
      "[157]\ttraining's multi_logloss: 0.603816\ttraining's f1: 0.296237\n",
      "[158]\ttraining's multi_logloss: 0.603815\ttraining's f1: 0.296237\n",
      "[159]\ttraining's multi_logloss: 0.603814\ttraining's f1: 0.296237\n",
      "[160]\ttraining's multi_logloss: 0.603813\ttraining's f1: 0.296237\n",
      "[161]\ttraining's multi_logloss: 0.603812\ttraining's f1: 0.296237\n",
      "[162]\ttraining's multi_logloss: 0.603812\ttraining's f1: 0.296237\n",
      "[163]\ttraining's multi_logloss: 0.603811\ttraining's f1: 0.296237\n",
      "[164]\ttraining's multi_logloss: 0.60381\ttraining's f1: 0.296237\n",
      "[165]\ttraining's multi_logloss: 0.603809\ttraining's f1: 0.296237\n",
      "[166]\ttraining's multi_logloss: 0.603808\ttraining's f1: 0.296237\n",
      "[167]\ttraining's multi_logloss: 0.603807\ttraining's f1: 0.296237\n",
      "[168]\ttraining's multi_logloss: 0.603806\ttraining's f1: 0.296237\n",
      "[169]\ttraining's multi_logloss: 0.603806\ttraining's f1: 0.296237\n",
      "[170]\ttraining's multi_logloss: 0.603805\ttraining's f1: 0.296237\n",
      "[171]\ttraining's multi_logloss: 0.603804\ttraining's f1: 0.296237\n",
      "[172]\ttraining's multi_logloss: 0.603803\ttraining's f1: 0.296237\n",
      "[173]\ttraining's multi_logloss: 0.603803\ttraining's f1: 0.296237\n",
      "[174]\ttraining's multi_logloss: 0.603802\ttraining's f1: 0.296237\n",
      "[175]\ttraining's multi_logloss: 0.603801\ttraining's f1: 0.296237\n",
      "[176]\ttraining's multi_logloss: 0.603801\ttraining's f1: 0.296237\n",
      "[177]\ttraining's multi_logloss: 0.6038\ttraining's f1: 0.297639\n",
      "[178]\ttraining's multi_logloss: 0.603799\ttraining's f1: 0.296237\n",
      "[179]\ttraining's multi_logloss: 0.603799\ttraining's f1: 0.297639\n",
      "[180]\ttraining's multi_logloss: 0.603798\ttraining's f1: 0.297639\n",
      "[181]\ttraining's multi_logloss: 0.603797\ttraining's f1: 0.296237\n",
      "[182]\ttraining's multi_logloss: 0.603797\ttraining's f1: 0.297639\n",
      "[183]\ttraining's multi_logloss: 0.603796\ttraining's f1: 0.297639\n",
      "[184]\ttraining's multi_logloss: 0.603795\ttraining's f1: 0.297639\n",
      "[185]\ttraining's multi_logloss: 0.603795\ttraining's f1: 0.297639\n",
      "[186]\ttraining's multi_logloss: 0.603794\ttraining's f1: 0.297639\n",
      "[187]\ttraining's multi_logloss: 0.603794\ttraining's f1: 0.297639\n",
      "[188]\ttraining's multi_logloss: 0.603793\ttraining's f1: 0.297639\n",
      "[189]\ttraining's multi_logloss: 0.603792\ttraining's f1: 0.297639\n",
      "[190]\ttraining's multi_logloss: 0.603792\ttraining's f1: 0.297639\n",
      "[191]\ttraining's multi_logloss: 0.603791\ttraining's f1: 0.297639\n",
      "[192]\ttraining's multi_logloss: 0.603791\ttraining's f1: 0.297639\n",
      "[193]\ttraining's multi_logloss: 0.60379\ttraining's f1: 0.296237\n",
      "[194]\ttraining's multi_logloss: 0.603789\ttraining's f1: 0.296237\n",
      "[195]\ttraining's multi_logloss: 0.603789\ttraining's f1: 0.296237\n",
      "[196]\ttraining's multi_logloss: 0.603788\ttraining's f1: 0.296237\n",
      "[197]\ttraining's multi_logloss: 0.603787\ttraining's f1: 0.296237\n",
      "[198]\ttraining's multi_logloss: 0.603787\ttraining's f1: 0.296237\n",
      "[199]\ttraining's multi_logloss: 0.603786\ttraining's f1: 0.297639\n",
      "[200]\ttraining's multi_logloss: 0.603785\ttraining's f1: 0.297639\n",
      "[201]\ttraining's multi_logloss: 0.603785\ttraining's f1: 0.297639\n",
      "[202]\ttraining's multi_logloss: 0.603784\ttraining's f1: 0.296237\n",
      "[203]\ttraining's multi_logloss: 0.603784\ttraining's f1: 0.296237\n",
      "[204]\ttraining's multi_logloss: 0.603783\ttraining's f1: 0.296237\n",
      "[205]\ttraining's multi_logloss: 0.603783\ttraining's f1: 0.296237\n",
      "[206]\ttraining's multi_logloss: 0.603782\ttraining's f1: 0.296702\n",
      "[207]\ttraining's multi_logloss: 0.603781\ttraining's f1: 0.296702\n",
      "[208]\ttraining's multi_logloss: 0.603781\ttraining's f1: 0.296702\n",
      "[209]\ttraining's multi_logloss: 0.60378\ttraining's f1: 0.296702\n",
      "[210]\ttraining's multi_logloss: 0.60378\ttraining's f1: 0.296702\n",
      "[211]\ttraining's multi_logloss: 0.603779\ttraining's f1: 0.296702\n",
      "[212]\ttraining's multi_logloss: 0.603779\ttraining's f1: 0.2981\n",
      "[213]\ttraining's multi_logloss: 0.603778\ttraining's f1: 0.2981\n",
      "[214]\ttraining's multi_logloss: 0.603777\ttraining's f1: 0.2981\n",
      "[215]\ttraining's multi_logloss: 0.603777\ttraining's f1: 0.2981\n",
      "[216]\ttraining's multi_logloss: 0.603777\ttraining's f1: 0.2981\n",
      "[217]\ttraining's multi_logloss: 0.603776\ttraining's f1: 0.2981\n",
      "[218]\ttraining's multi_logloss: 0.603776\ttraining's f1: 0.2981\n",
      "[219]\ttraining's multi_logloss: 0.603775\ttraining's f1: 0.2981\n",
      "[220]\ttraining's multi_logloss: 0.603775\ttraining's f1: 0.2981\n",
      "[221]\ttraining's multi_logloss: 0.603774\ttraining's f1: 0.2981\n",
      "[222]\ttraining's multi_logloss: 0.603774\ttraining's f1: 0.2981\n",
      "[223]\ttraining's multi_logloss: 0.603773\ttraining's f1: 0.2981\n",
      "[224]\ttraining's multi_logloss: 0.603773\ttraining's f1: 0.2981\n",
      "[225]\ttraining's multi_logloss: 0.603772\ttraining's f1: 0.2981\n",
      "[226]\ttraining's multi_logloss: 0.603772\ttraining's f1: 0.2981\n",
      "[227]\ttraining's multi_logloss: 0.603771\ttraining's f1: 0.2981\n",
      "[228]\ttraining's multi_logloss: 0.603771\ttraining's f1: 0.2981\n",
      "[229]\ttraining's multi_logloss: 0.60377\ttraining's f1: 0.2981\n",
      "[230]\ttraining's multi_logloss: 0.60377\ttraining's f1: 0.2981\n",
      "[231]\ttraining's multi_logloss: 0.603769\ttraining's f1: 0.2981\n",
      "[232]\ttraining's multi_logloss: 0.603769\ttraining's f1: 0.2981\n",
      "[233]\ttraining's multi_logloss: 0.603769\ttraining's f1: 0.2981\n",
      "[234]\ttraining's multi_logloss: 0.603768\ttraining's f1: 0.2981\n",
      "[235]\ttraining's multi_logloss: 0.603768\ttraining's f1: 0.2981\n",
      "[236]\ttraining's multi_logloss: 0.603767\ttraining's f1: 0.2981\n",
      "[237]\ttraining's multi_logloss: 0.603767\ttraining's f1: 0.296702\n",
      "[238]\ttraining's multi_logloss: 0.603766\ttraining's f1: 0.296702\n",
      "[239]\ttraining's multi_logloss: 0.603766\ttraining's f1: 0.296702\n",
      "[240]\ttraining's multi_logloss: 0.603766\ttraining's f1: 0.296702\n",
      "[241]\ttraining's multi_logloss: 0.603765\ttraining's f1: 0.2981\n",
      "[242]\ttraining's multi_logloss: 0.603765\ttraining's f1: 0.296702\n",
      "[243]\ttraining's multi_logloss: 0.603764\ttraining's f1: 0.2981\n",
      "[244]\ttraining's multi_logloss: 0.603764\ttraining's f1: 0.2981\n",
      "[245]\ttraining's multi_logloss: 0.603763\ttraining's f1: 0.2981\n",
      "[246]\ttraining's multi_logloss: 0.603763\ttraining's f1: 0.2981\n",
      "[247]\ttraining's multi_logloss: 0.603762\ttraining's f1: 0.2981\n",
      "[248]\ttraining's multi_logloss: 0.603762\ttraining's f1: 0.2981\n",
      "[249]\ttraining's multi_logloss: 0.603762\ttraining's f1: 0.2981\n",
      "[250]\ttraining's multi_logloss: 0.603761\ttraining's f1: 0.2981\n",
      "[251]\ttraining's multi_logloss: 0.603761\ttraining's f1: 0.2981\n",
      "[252]\ttraining's multi_logloss: 0.60376\ttraining's f1: 0.296702\n",
      "[253]\ttraining's multi_logloss: 0.60376\ttraining's f1: 0.296702\n",
      "[254]\ttraining's multi_logloss: 0.603759\ttraining's f1: 0.296702\n",
      "[255]\ttraining's multi_logloss: 0.603759\ttraining's f1: 0.2981\n",
      "[256]\ttraining's multi_logloss: 0.603759\ttraining's f1: 0.2981\n",
      "[257]\ttraining's multi_logloss: 0.603758\ttraining's f1: 0.296702\n",
      "[258]\ttraining's multi_logloss: 0.603758\ttraining's f1: 0.296702\n",
      "[259]\ttraining's multi_logloss: 0.603757\ttraining's f1: 0.2981\n",
      "[260]\ttraining's multi_logloss: 0.603757\ttraining's f1: 0.296702\n",
      "[261]\ttraining's multi_logloss: 0.603756\ttraining's f1: 0.2981\n",
      "[262]\ttraining's multi_logloss: 0.603756\ttraining's f1: 0.2981\n",
      "[263]\ttraining's multi_logloss: 0.603756\ttraining's f1: 0.2981\n",
      "[264]\ttraining's multi_logloss: 0.603755\ttraining's f1: 0.2981\n",
      "[265]\ttraining's multi_logloss: 0.603755\ttraining's f1: 0.2981\n",
      "[266]\ttraining's multi_logloss: 0.603755\ttraining's f1: 0.2981\n",
      "[267]\ttraining's multi_logloss: 0.603755\ttraining's f1: 0.2981\n",
      "[268]\ttraining's multi_logloss: 0.603754\ttraining's f1: 0.2981\n",
      "[269]\ttraining's multi_logloss: 0.603754\ttraining's f1: 0.2981\n",
      "[270]\ttraining's multi_logloss: 0.603753\ttraining's f1: 0.2981\n",
      "[271]\ttraining's multi_logloss: 0.603753\ttraining's f1: 0.2981\n",
      "[272]\ttraining's multi_logloss: 0.603753\ttraining's f1: 0.2981\n",
      "[273]\ttraining's multi_logloss: 0.603752\ttraining's f1: 0.2981\n",
      "[274]\ttraining's multi_logloss: 0.603752\ttraining's f1: 0.2981\n",
      "[275]\ttraining's multi_logloss: 0.603752\ttraining's f1: 0.2981\n",
      "[276]\ttraining's multi_logloss: 0.603751\ttraining's f1: 0.2981\n",
      "[277]\ttraining's multi_logloss: 0.603751\ttraining's f1: 0.2981\n",
      "[278]\ttraining's multi_logloss: 0.603751\ttraining's f1: 0.2981\n",
      "[279]\ttraining's multi_logloss: 0.60375\ttraining's f1: 0.2981\n",
      "[280]\ttraining's multi_logloss: 0.60375\ttraining's f1: 0.2981\n",
      "[281]\ttraining's multi_logloss: 0.60375\ttraining's f1: 0.2981\n",
      "[282]\ttraining's multi_logloss: 0.603749\ttraining's f1: 0.2981\n",
      "[283]\ttraining's multi_logloss: 0.603749\ttraining's f1: 0.2981\n",
      "[284]\ttraining's multi_logloss: 0.603749\ttraining's f1: 0.2981\n",
      "[285]\ttraining's multi_logloss: 0.603748\ttraining's f1: 0.2981\n",
      "[286]\ttraining's multi_logloss: 0.603748\ttraining's f1: 0.296702\n",
      "[287]\ttraining's multi_logloss: 0.603748\ttraining's f1: 0.296702\n",
      "[288]\ttraining's multi_logloss: 0.603747\ttraining's f1: 0.296702\n",
      "[289]\ttraining's multi_logloss: 0.603747\ttraining's f1: 0.296702\n",
      "[290]\ttraining's multi_logloss: 0.603747\ttraining's f1: 0.296702\n",
      "[291]\ttraining's multi_logloss: 0.603746\ttraining's f1: 0.296702\n",
      "[292]\ttraining's multi_logloss: 0.603746\ttraining's f1: 0.296702\n",
      "[293]\ttraining's multi_logloss: 0.603746\ttraining's f1: 0.2981\n",
      "[294]\ttraining's multi_logloss: 0.603745\ttraining's f1: 0.2981\n",
      "[295]\ttraining's multi_logloss: 0.603745\ttraining's f1: 0.2981\n",
      "[296]\ttraining's multi_logloss: 0.603745\ttraining's f1: 0.2981\n",
      "[297]\ttraining's multi_logloss: 0.603744\ttraining's f1: 0.2981\n",
      "[298]\ttraining's multi_logloss: 0.603744\ttraining's f1: 0.2981\n",
      "[299]\ttraining's multi_logloss: 0.603744\ttraining's f1: 0.2981\n",
      "[300]\ttraining's multi_logloss: 0.603744\ttraining's f1: 0.2981\n",
      "[301]\ttraining's multi_logloss: 0.603743\ttraining's f1: 0.2981\n",
      "[302]\ttraining's multi_logloss: 0.603743\ttraining's f1: 0.2981\n",
      "[303]\ttraining's multi_logloss: 0.603743\ttraining's f1: 0.2981\n",
      "[304]\ttraining's multi_logloss: 0.603742\ttraining's f1: 0.2981\n",
      "[305]\ttraining's multi_logloss: 0.603742\ttraining's f1: 0.2981\n",
      "[306]\ttraining's multi_logloss: 0.603742\ttraining's f1: 0.2981\n",
      "[307]\ttraining's multi_logloss: 0.603741\ttraining's f1: 0.296702\n",
      "[308]\ttraining's multi_logloss: 0.603741\ttraining's f1: 0.2981\n",
      "[309]\ttraining's multi_logloss: 0.603741\ttraining's f1: 0.2981\n",
      "[310]\ttraining's multi_logloss: 0.603741\ttraining's f1: 0.2981\n",
      "[311]\ttraining's multi_logloss: 0.60374\ttraining's f1: 0.2981\n",
      "[312]\ttraining's multi_logloss: 0.60374\ttraining's f1: 0.2981\n",
      "[313]\ttraining's multi_logloss: 0.60374\ttraining's f1: 0.2981\n",
      "[314]\ttraining's multi_logloss: 0.60374\ttraining's f1: 0.2981\n",
      "[315]\ttraining's multi_logloss: 0.603739\ttraining's f1: 0.2981\n",
      "[316]\ttraining's multi_logloss: 0.603739\ttraining's f1: 0.2981\n",
      "[317]\ttraining's multi_logloss: 0.603739\ttraining's f1: 0.2981\n",
      "[318]\ttraining's multi_logloss: 0.603738\ttraining's f1: 0.2981\n",
      "[319]\ttraining's multi_logloss: 0.603738\ttraining's f1: 0.2981\n",
      "[320]\ttraining's multi_logloss: 0.603738\ttraining's f1: 0.2981\n",
      "[321]\ttraining's multi_logloss: 0.603738\ttraining's f1: 0.2981\n",
      "[322]\ttraining's multi_logloss: 0.603737\ttraining's f1: 0.2981\n",
      "[323]\ttraining's multi_logloss: 0.603737\ttraining's f1: 0.2981\n",
      "[324]\ttraining's multi_logloss: 0.603737\ttraining's f1: 0.2981\n",
      "[325]\ttraining's multi_logloss: 0.603737\ttraining's f1: 0.2981\n",
      "[326]\ttraining's multi_logloss: 0.603736\ttraining's f1: 0.2981\n",
      "[327]\ttraining's multi_logloss: 0.603736\ttraining's f1: 0.2981\n",
      "[328]\ttraining's multi_logloss: 0.603736\ttraining's f1: 0.2981\n",
      "[329]\ttraining's multi_logloss: 0.603736\ttraining's f1: 0.2981\n",
      "[330]\ttraining's multi_logloss: 0.603735\ttraining's f1: 0.2981\n",
      "[331]\ttraining's multi_logloss: 0.603735\ttraining's f1: 0.2981\n",
      "[332]\ttraining's multi_logloss: 0.603735\ttraining's f1: 0.2981\n",
      "[333]\ttraining's multi_logloss: 0.603734\ttraining's f1: 0.2981\n",
      "[334]\ttraining's multi_logloss: 0.603734\ttraining's f1: 0.2981\n",
      "[335]\ttraining's multi_logloss: 0.603734\ttraining's f1: 0.2981\n",
      "[336]\ttraining's multi_logloss: 0.603734\ttraining's f1: 0.2981\n",
      "[337]\ttraining's multi_logloss: 0.603733\ttraining's f1: 0.2981\n",
      "[338]\ttraining's multi_logloss: 0.603733\ttraining's f1: 0.2981\n",
      "[339]\ttraining's multi_logloss: 0.603733\ttraining's f1: 0.2981\n",
      "[340]\ttraining's multi_logloss: 0.603733\ttraining's f1: 0.2981\n",
      "[341]\ttraining's multi_logloss: 0.603733\ttraining's f1: 0.2981\n",
      "[342]\ttraining's multi_logloss: 0.603732\ttraining's f1: 0.2981\n",
      "[343]\ttraining's multi_logloss: 0.603732\ttraining's f1: 0.2981\n",
      "[344]\ttraining's multi_logloss: 0.603732\ttraining's f1: 0.2981\n",
      "[345]\ttraining's multi_logloss: 0.603732\ttraining's f1: 0.2981\n",
      "[346]\ttraining's multi_logloss: 0.603732\ttraining's f1: 0.2981\n",
      "[347]\ttraining's multi_logloss: 0.603731\ttraining's f1: 0.2981\n",
      "[348]\ttraining's multi_logloss: 0.603731\ttraining's f1: 0.2981\n",
      "[349]\ttraining's multi_logloss: 0.603731\ttraining's f1: 0.2981\n",
      "[350]\ttraining's multi_logloss: 0.60373\ttraining's f1: 0.2981\n",
      "[351]\ttraining's multi_logloss: 0.60373\ttraining's f1: 0.296702\n",
      "[352]\ttraining's multi_logloss: 0.60373\ttraining's f1: 0.296702\n",
      "[353]\ttraining's multi_logloss: 0.60373\ttraining's f1: 0.296702\n",
      "[354]\ttraining's multi_logloss: 0.603729\ttraining's f1: 0.296702\n",
      "[355]\ttraining's multi_logloss: 0.603729\ttraining's f1: 0.296702\n",
      "[356]\ttraining's multi_logloss: 0.603729\ttraining's f1: 0.296702\n",
      "[357]\ttraining's multi_logloss: 0.603729\ttraining's f1: 0.296702\n",
      "[358]\ttraining's multi_logloss: 0.603729\ttraining's f1: 0.296702\n",
      "[359]\ttraining's multi_logloss: 0.603728\ttraining's f1: 0.296702\n",
      "[360]\ttraining's multi_logloss: 0.603728\ttraining's f1: 0.296702\n",
      "[361]\ttraining's multi_logloss: 0.603728\ttraining's f1: 0.296702\n",
      "[362]\ttraining's multi_logloss: 0.603728\ttraining's f1: 0.296702\n",
      "[363]\ttraining's multi_logloss: 0.603727\ttraining's f1: 0.296702\n",
      "[364]\ttraining's multi_logloss: 0.603727\ttraining's f1: 0.296702\n",
      "[365]\ttraining's multi_logloss: 0.603727\ttraining's f1: 0.296702\n",
      "[366]\ttraining's multi_logloss: 0.603727\ttraining's f1: 0.296702\n",
      "[367]\ttraining's multi_logloss: 0.603726\ttraining's f1: 0.296702\n",
      "[368]\ttraining's multi_logloss: 0.603726\ttraining's f1: 0.296702\n",
      "[369]\ttraining's multi_logloss: 0.603726\ttraining's f1: 0.296702\n",
      "[370]\ttraining's multi_logloss: 0.603726\ttraining's f1: 0.296702\n",
      "[371]\ttraining's multi_logloss: 0.603726\ttraining's f1: 0.296702\n",
      "[372]\ttraining's multi_logloss: 0.603725\ttraining's f1: 0.296702\n",
      "[373]\ttraining's multi_logloss: 0.603725\ttraining's f1: 0.2981\n",
      "[374]\ttraining's multi_logloss: 0.603725\ttraining's f1: 0.2981\n",
      "[375]\ttraining's multi_logloss: 0.603725\ttraining's f1: 0.2981\n",
      "[376]\ttraining's multi_logloss: 0.603725\ttraining's f1: 0.2981\n",
      "[377]\ttraining's multi_logloss: 0.603724\ttraining's f1: 0.2981\n",
      "[378]\ttraining's multi_logloss: 0.603724\ttraining's f1: 0.2981\n",
      "[379]\ttraining's multi_logloss: 0.603724\ttraining's f1: 0.296702\n",
      "[380]\ttraining's multi_logloss: 0.603724\ttraining's f1: 0.2981\n",
      "[381]\ttraining's multi_logloss: 0.603724\ttraining's f1: 0.296702\n",
      "[382]\ttraining's multi_logloss: 0.603723\ttraining's f1: 0.296702\n",
      "[383]\ttraining's multi_logloss: 0.603723\ttraining's f1: 0.2981\n",
      "[384]\ttraining's multi_logloss: 0.603723\ttraining's f1: 0.2981\n",
      "[385]\ttraining's multi_logloss: 0.603723\ttraining's f1: 0.296702\n",
      "[386]\ttraining's multi_logloss: 0.603723\ttraining's f1: 0.2981\n",
      "[387]\ttraining's multi_logloss: 0.603723\ttraining's f1: 0.2981\n",
      "[388]\ttraining's multi_logloss: 0.603722\ttraining's f1: 0.2981\n",
      "[389]\ttraining's multi_logloss: 0.603722\ttraining's f1: 0.2981\n",
      "[390]\ttraining's multi_logloss: 0.603722\ttraining's f1: 0.2981\n",
      "[391]\ttraining's multi_logloss: 0.603722\ttraining's f1: 0.2981\n",
      "[392]\ttraining's multi_logloss: 0.603722\ttraining's f1: 0.2981\n",
      "[393]\ttraining's multi_logloss: 0.603722\ttraining's f1: 0.2981\n",
      "[394]\ttraining's multi_logloss: 0.603721\ttraining's f1: 0.2981\n",
      "[395]\ttraining's multi_logloss: 0.603721\ttraining's f1: 0.296702\n",
      "[396]\ttraining's multi_logloss: 0.603721\ttraining's f1: 0.296702\n",
      "[397]\ttraining's multi_logloss: 0.603721\ttraining's f1: 0.2981\n",
      "[398]\ttraining's multi_logloss: 0.603721\ttraining's f1: 0.2981\n",
      "[399]\ttraining's multi_logloss: 0.60372\ttraining's f1: 0.2981\n",
      "[400]\ttraining's multi_logloss: 0.60372\ttraining's f1: 0.2981\n",
      "[401]\ttraining's multi_logloss: 0.60372\ttraining's f1: 0.2981\n",
      "[402]\ttraining's multi_logloss: 0.60372\ttraining's f1: 0.2981\n",
      "[403]\ttraining's multi_logloss: 0.60372\ttraining's f1: 0.2981\n",
      "[404]\ttraining's multi_logloss: 0.60372\ttraining's f1: 0.2981\n",
      "[405]\ttraining's multi_logloss: 0.603719\ttraining's f1: 0.2981\n",
      "[406]\ttraining's multi_logloss: 0.603719\ttraining's f1: 0.2981\n",
      "[407]\ttraining's multi_logloss: 0.603719\ttraining's f1: 0.2981\n",
      "[408]\ttraining's multi_logloss: 0.603719\ttraining's f1: 0.2981\n",
      "[409]\ttraining's multi_logloss: 0.603719\ttraining's f1: 0.2981\n",
      "[410]\ttraining's multi_logloss: 0.603718\ttraining's f1: 0.2981\n",
      "[411]\ttraining's multi_logloss: 0.603718\ttraining's f1: 0.2981\n",
      "[412]\ttraining's multi_logloss: 0.603718\ttraining's f1: 0.2981\n",
      "[413]\ttraining's multi_logloss: 0.603718\ttraining's f1: 0.2981\n",
      "[414]\ttraining's multi_logloss: 0.603718\ttraining's f1: 0.2981\n",
      "[415]\ttraining's multi_logloss: 0.603718\ttraining's f1: 0.296702\n",
      "[416]\ttraining's multi_logloss: 0.603717\ttraining's f1: 0.296702\n",
      "[417]\ttraining's multi_logloss: 0.603717\ttraining's f1: 0.296702\n",
      "[418]\ttraining's multi_logloss: 0.603717\ttraining's f1: 0.296702\n",
      "[419]\ttraining's multi_logloss: 0.603717\ttraining's f1: 0.296702\n",
      "[420]\ttraining's multi_logloss: 0.603717\ttraining's f1: 0.296702\n",
      "[421]\ttraining's multi_logloss: 0.603716\ttraining's f1: 0.296702\n",
      "[422]\ttraining's multi_logloss: 0.603716\ttraining's f1: 0.296702\n",
      "[423]\ttraining's multi_logloss: 0.603716\ttraining's f1: 0.296702\n",
      "[424]\ttraining's multi_logloss: 0.603716\ttraining's f1: 0.296702\n",
      "[425]\ttraining's multi_logloss: 0.603716\ttraining's f1: 0.296702\n",
      "[426]\ttraining's multi_logloss: 0.603716\ttraining's f1: 0.296702\n",
      "[427]\ttraining's multi_logloss: 0.603715\ttraining's f1: 0.296702\n",
      "[428]\ttraining's multi_logloss: 0.603715\ttraining's f1: 0.296702\n",
      "[429]\ttraining's multi_logloss: 0.603715\ttraining's f1: 0.296702\n",
      "[430]\ttraining's multi_logloss: 0.603715\ttraining's f1: 0.296702\n",
      "[431]\ttraining's multi_logloss: 0.603715\ttraining's f1: 0.296702\n",
      "[432]\ttraining's multi_logloss: 0.603715\ttraining's f1: 0.296702\n",
      "[433]\ttraining's multi_logloss: 0.603715\ttraining's f1: 0.296702\n",
      "[434]\ttraining's multi_logloss: 0.603714\ttraining's f1: 0.296702\n",
      "[435]\ttraining's multi_logloss: 0.603714\ttraining's f1: 0.296702\n",
      "[436]\ttraining's multi_logloss: 0.603714\ttraining's f1: 0.296702\n",
      "[437]\ttraining's multi_logloss: 0.603714\ttraining's f1: 0.296702\n",
      "[438]\ttraining's multi_logloss: 0.603714\ttraining's f1: 0.296702\n",
      "[439]\ttraining's multi_logloss: 0.603714\ttraining's f1: 0.296702\n",
      "[440]\ttraining's multi_logloss: 0.603713\ttraining's f1: 0.296702\n",
      "[441]\ttraining's multi_logloss: 0.603713\ttraining's f1: 0.296702\n",
      "[442]\ttraining's multi_logloss: 0.603713\ttraining's f1: 0.296702\n",
      "[443]\ttraining's multi_logloss: 0.603713\ttraining's f1: 0.296702\n",
      "[444]\ttraining's multi_logloss: 0.603713\ttraining's f1: 0.296702\n",
      "[445]\ttraining's multi_logloss: 0.603713\ttraining's f1: 0.296702\n",
      "[446]\ttraining's multi_logloss: 0.603713\ttraining's f1: 0.296702\n",
      "[447]\ttraining's multi_logloss: 0.603712\ttraining's f1: 0.296702\n",
      "[448]\ttraining's multi_logloss: 0.603712\ttraining's f1: 0.296702\n",
      "[449]\ttraining's multi_logloss: 0.603712\ttraining's f1: 0.296702\n",
      "[450]\ttraining's multi_logloss: 0.603712\ttraining's f1: 0.296702\n",
      "[451]\ttraining's multi_logloss: 0.603712\ttraining's f1: 0.296702\n",
      "[452]\ttraining's multi_logloss: 0.603712\ttraining's f1: 0.296702\n",
      "[453]\ttraining's multi_logloss: 0.603711\ttraining's f1: 0.296702\n",
      "[454]\ttraining's multi_logloss: 0.603711\ttraining's f1: 0.296702\n",
      "[455]\ttraining's multi_logloss: 0.603711\ttraining's f1: 0.296702\n",
      "[456]\ttraining's multi_logloss: 0.603711\ttraining's f1: 0.296702\n",
      "[457]\ttraining's multi_logloss: 0.603711\ttraining's f1: 0.296702\n",
      "[458]\ttraining's multi_logloss: 0.603711\ttraining's f1: 0.296702\n",
      "[459]\ttraining's multi_logloss: 0.603711\ttraining's f1: 0.296702\n",
      "[460]\ttraining's multi_logloss: 0.603711\ttraining's f1: 0.296702\n",
      "[461]\ttraining's multi_logloss: 0.60371\ttraining's f1: 0.296702\n",
      "[462]\ttraining's multi_logloss: 0.60371\ttraining's f1: 0.296702\n",
      "[463]\ttraining's multi_logloss: 0.60371\ttraining's f1: 0.296702\n",
      "[464]\ttraining's multi_logloss: 0.60371\ttraining's f1: 0.296702\n",
      "[465]\ttraining's multi_logloss: 0.60371\ttraining's f1: 0.296702\n",
      "[466]\ttraining's multi_logloss: 0.60371\ttraining's f1: 0.296702\n",
      "[467]\ttraining's multi_logloss: 0.60371\ttraining's f1: 0.296702\n",
      "[468]\ttraining's multi_logloss: 0.603709\ttraining's f1: 0.296702\n",
      "[469]\ttraining's multi_logloss: 0.603709\ttraining's f1: 0.296702\n",
      "[470]\ttraining's multi_logloss: 0.603709\ttraining's f1: 0.296702\n",
      "[471]\ttraining's multi_logloss: 0.603709\ttraining's f1: 0.296702\n",
      "[472]\ttraining's multi_logloss: 0.603709\ttraining's f1: 0.296702\n",
      "[473]\ttraining's multi_logloss: 0.603709\ttraining's f1: 0.296702\n",
      "[474]\ttraining's multi_logloss: 0.603709\ttraining's f1: 0.296702\n",
      "[475]\ttraining's multi_logloss: 0.603709\ttraining's f1: 0.296702\n",
      "[476]\ttraining's multi_logloss: 0.603708\ttraining's f1: 0.296702\n",
      "[477]\ttraining's multi_logloss: 0.603708\ttraining's f1: 0.296702\n",
      "[478]\ttraining's multi_logloss: 0.603708\ttraining's f1: 0.296702\n",
      "[479]\ttraining's multi_logloss: 0.603708\ttraining's f1: 0.296702\n",
      "[480]\ttraining's multi_logloss: 0.603708\ttraining's f1: 0.296702\n",
      "[481]\ttraining's multi_logloss: 0.603708\ttraining's f1: 0.296702\n",
      "[482]\ttraining's multi_logloss: 0.603708\ttraining's f1: 0.296702\n",
      "[483]\ttraining's multi_logloss: 0.603707\ttraining's f1: 0.296702\n",
      "[484]\ttraining's multi_logloss: 0.603707\ttraining's f1: 0.296702\n",
      "[485]\ttraining's multi_logloss: 0.603707\ttraining's f1: 0.296702\n",
      "[486]\ttraining's multi_logloss: 0.603707\ttraining's f1: 0.296702\n",
      "[487]\ttraining's multi_logloss: 0.603707\ttraining's f1: 0.296702\n",
      "[488]\ttraining's multi_logloss: 0.603707\ttraining's f1: 0.296702\n",
      "[489]\ttraining's multi_logloss: 0.603707\ttraining's f1: 0.296702\n",
      "[490]\ttraining's multi_logloss: 0.603707\ttraining's f1: 0.296702\n",
      "[491]\ttraining's multi_logloss: 0.603706\ttraining's f1: 0.296702\n",
      "[492]\ttraining's multi_logloss: 0.603706\ttraining's f1: 0.296702\n",
      "[493]\ttraining's multi_logloss: 0.603706\ttraining's f1: 0.296702\n",
      "[494]\ttraining's multi_logloss: 0.603706\ttraining's f1: 0.296702\n",
      "[495]\ttraining's multi_logloss: 0.603706\ttraining's f1: 0.296702\n",
      "[496]\ttraining's multi_logloss: 0.603706\ttraining's f1: 0.296702\n",
      "[497]\ttraining's multi_logloss: 0.603706\ttraining's f1: 0.296702\n",
      "[498]\ttraining's multi_logloss: 0.603706\ttraining's f1: 0.296702\n",
      "[499]\ttraining's multi_logloss: 0.603705\ttraining's f1: 0.296702\n",
      "[500]\ttraining's multi_logloss: 0.603705\ttraining's f1: 0.296702\n",
      "[501]\ttraining's multi_logloss: 0.603705\ttraining's f1: 0.296702\n",
      "[502]\ttraining's multi_logloss: 0.603705\ttraining's f1: 0.296702\n",
      "[503]\ttraining's multi_logloss: 0.603705\ttraining's f1: 0.296702\n",
      "[504]\ttraining's multi_logloss: 0.603705\ttraining's f1: 0.296702\n",
      "[505]\ttraining's multi_logloss: 0.603705\ttraining's f1: 0.296702\n",
      "[506]\ttraining's multi_logloss: 0.603705\ttraining's f1: 0.296702\n",
      "[507]\ttraining's multi_logloss: 0.603704\ttraining's f1: 0.296702\n",
      "[508]\ttraining's multi_logloss: 0.603704\ttraining's f1: 0.296702\n",
      "[509]\ttraining's multi_logloss: 0.603704\ttraining's f1: 0.296702\n",
      "[510]\ttraining's multi_logloss: 0.603704\ttraining's f1: 0.296702\n",
      "[511]\ttraining's multi_logloss: 0.603704\ttraining's f1: 0.296702\n",
      "[512]\ttraining's multi_logloss: 0.603704\ttraining's f1: 0.296702\n",
      "[513]\ttraining's multi_logloss: 0.603704\ttraining's f1: 0.296702\n",
      "[514]\ttraining's multi_logloss: 0.603704\ttraining's f1: 0.296702\n",
      "[515]\ttraining's multi_logloss: 0.603704\ttraining's f1: 0.296702\n",
      "[516]\ttraining's multi_logloss: 0.603703\ttraining's f1: 0.296702\n",
      "[517]\ttraining's multi_logloss: 0.603703\ttraining's f1: 0.296702\n",
      "[518]\ttraining's multi_logloss: 0.603703\ttraining's f1: 0.296702\n",
      "[519]\ttraining's multi_logloss: 0.603703\ttraining's f1: 0.296702\n",
      "[520]\ttraining's multi_logloss: 0.603703\ttraining's f1: 0.296702\n",
      "[521]\ttraining's multi_logloss: 0.603703\ttraining's f1: 0.296702\n",
      "[522]\ttraining's multi_logloss: 0.603703\ttraining's f1: 0.296702\n",
      "[523]\ttraining's multi_logloss: 0.603703\ttraining's f1: 0.296702\n",
      "[524]\ttraining's multi_logloss: 0.603703\ttraining's f1: 0.296702\n",
      "[525]\ttraining's multi_logloss: 0.603702\ttraining's f1: 0.296702\n",
      "[526]\ttraining's multi_logloss: 0.603702\ttraining's f1: 0.296702\n",
      "[527]\ttraining's multi_logloss: 0.603702\ttraining's f1: 0.296702\n",
      "[528]\ttraining's multi_logloss: 0.603702\ttraining's f1: 0.296702\n",
      "[529]\ttraining's multi_logloss: 0.603702\ttraining's f1: 0.296702\n",
      "[530]\ttraining's multi_logloss: 0.603702\ttraining's f1: 0.296702\n",
      "[531]\ttraining's multi_logloss: 0.603702\ttraining's f1: 0.296702\n",
      "[532]\ttraining's multi_logloss: 0.603702\ttraining's f1: 0.296702\n",
      "[533]\ttraining's multi_logloss: 0.603701\ttraining's f1: 0.296702\n",
      "[534]\ttraining's multi_logloss: 0.603701\ttraining's f1: 0.296702\n",
      "[535]\ttraining's multi_logloss: 0.603701\ttraining's f1: 0.296702\n",
      "[536]\ttraining's multi_logloss: 0.603701\ttraining's f1: 0.296702\n",
      "[537]\ttraining's multi_logloss: 0.603701\ttraining's f1: 0.296702\n",
      "[538]\ttraining's multi_logloss: 0.603701\ttraining's f1: 0.296702\n",
      "[539]\ttraining's multi_logloss: 0.603701\ttraining's f1: 0.296702\n",
      "[540]\ttraining's multi_logloss: 0.603701\ttraining's f1: 0.296702\n",
      "[541]\ttraining's multi_logloss: 0.603701\ttraining's f1: 0.296702\n",
      "[542]\ttraining's multi_logloss: 0.6037\ttraining's f1: 0.296702\n",
      "[543]\ttraining's multi_logloss: 0.6037\ttraining's f1: 0.296702\n",
      "[544]\ttraining's multi_logloss: 0.6037\ttraining's f1: 0.296702\n",
      "[545]\ttraining's multi_logloss: 0.6037\ttraining's f1: 0.296702\n",
      "[546]\ttraining's multi_logloss: 0.6037\ttraining's f1: 0.296702\n",
      "[547]\ttraining's multi_logloss: 0.6037\ttraining's f1: 0.296702\n",
      "[548]\ttraining's multi_logloss: 0.6037\ttraining's f1: 0.296702\n",
      "[549]\ttraining's multi_logloss: 0.6037\ttraining's f1: 0.296702\n",
      "[550]\ttraining's multi_logloss: 0.6037\ttraining's f1: 0.296702\n",
      "[551]\ttraining's multi_logloss: 0.603699\ttraining's f1: 0.296702\n",
      "[552]\ttraining's multi_logloss: 0.603699\ttraining's f1: 0.296702\n",
      "[553]\ttraining's multi_logloss: 0.603699\ttraining's f1: 0.296702\n",
      "[554]\ttraining's multi_logloss: 0.603699\ttraining's f1: 0.296702\n",
      "[555]\ttraining's multi_logloss: 0.603699\ttraining's f1: 0.296702\n",
      "[556]\ttraining's multi_logloss: 0.603699\ttraining's f1: 0.296702\n",
      "[557]\ttraining's multi_logloss: 0.603699\ttraining's f1: 0.296702\n",
      "[558]\ttraining's multi_logloss: 0.603699\ttraining's f1: 0.296702\n",
      "[559]\ttraining's multi_logloss: 0.603699\ttraining's f1: 0.296702\n",
      "[560]\ttraining's multi_logloss: 0.603698\ttraining's f1: 0.296702\n",
      "[561]\ttraining's multi_logloss: 0.603698\ttraining's f1: 0.296702\n",
      "[562]\ttraining's multi_logloss: 0.603698\ttraining's f1: 0.296702\n",
      "[563]\ttraining's multi_logloss: 0.603698\ttraining's f1: 0.296702\n",
      "[564]\ttraining's multi_logloss: 0.603698\ttraining's f1: 0.296702\n",
      "[565]\ttraining's multi_logloss: 0.603698\ttraining's f1: 0.296702\n",
      "[566]\ttraining's multi_logloss: 0.603698\ttraining's f1: 0.296702\n",
      "[567]\ttraining's multi_logloss: 0.603698\ttraining's f1: 0.296702\n",
      "[568]\ttraining's multi_logloss: 0.603698\ttraining's f1: 0.296702\n",
      "[569]\ttraining's multi_logloss: 0.603698\ttraining's f1: 0.296702\n",
      "[570]\ttraining's multi_logloss: 0.603698\ttraining's f1: 0.296702\n",
      "[571]\ttraining's multi_logloss: 0.603697\ttraining's f1: 0.296702\n",
      "[572]\ttraining's multi_logloss: 0.603697\ttraining's f1: 0.296702\n",
      "[573]\ttraining's multi_logloss: 0.603697\ttraining's f1: 0.296702\n",
      "[574]\ttraining's multi_logloss: 0.603697\ttraining's f1: 0.296702\n",
      "[575]\ttraining's multi_logloss: 0.603697\ttraining's f1: 0.296702\n",
      "[576]\ttraining's multi_logloss: 0.603697\ttraining's f1: 0.296702\n",
      "[577]\ttraining's multi_logloss: 0.603697\ttraining's f1: 0.296702\n",
      "[578]\ttraining's multi_logloss: 0.603697\ttraining's f1: 0.296702\n",
      "[579]\ttraining's multi_logloss: 0.603697\ttraining's f1: 0.296702\n",
      "[580]\ttraining's multi_logloss: 0.603697\ttraining's f1: 0.296702\n",
      "[581]\ttraining's multi_logloss: 0.603696\ttraining's f1: 0.296702\n",
      "[582]\ttraining's multi_logloss: 0.603696\ttraining's f1: 0.296702\n",
      "[583]\ttraining's multi_logloss: 0.603696\ttraining's f1: 0.296702\n",
      "[584]\ttraining's multi_logloss: 0.603696\ttraining's f1: 0.296702\n",
      "[585]\ttraining's multi_logloss: 0.603696\ttraining's f1: 0.296702\n",
      "[586]\ttraining's multi_logloss: 0.603696\ttraining's f1: 0.296702\n",
      "[587]\ttraining's multi_logloss: 0.603696\ttraining's f1: 0.296702\n",
      "[588]\ttraining's multi_logloss: 0.603696\ttraining's f1: 0.296702\n",
      "[589]\ttraining's multi_logloss: 0.603696\ttraining's f1: 0.296702\n",
      "[590]\ttraining's multi_logloss: 0.603696\ttraining's f1: 0.296702\n",
      "[591]\ttraining's multi_logloss: 0.603696\ttraining's f1: 0.296702\n",
      "[592]\ttraining's multi_logloss: 0.603695\ttraining's f1: 0.296702\n",
      "[593]\ttraining's multi_logloss: 0.603695\ttraining's f1: 0.296702\n",
      "[594]\ttraining's multi_logloss: 0.603695\ttraining's f1: 0.296702\n",
      "[595]\ttraining's multi_logloss: 0.603695\ttraining's f1: 0.296702\n",
      "[596]\ttraining's multi_logloss: 0.603695\ttraining's f1: 0.296702\n",
      "[597]\ttraining's multi_logloss: 0.603695\ttraining's f1: 0.296702\n",
      "[598]\ttraining's multi_logloss: 0.603695\ttraining's f1: 0.296702\n",
      "[599]\ttraining's multi_logloss: 0.603695\ttraining's f1: 0.296702\n",
      "[600]\ttraining's multi_logloss: 0.603695\ttraining's f1: 0.296702\n",
      "[601]\ttraining's multi_logloss: 0.603695\ttraining's f1: 0.296702\n",
      "[602]\ttraining's multi_logloss: 0.603695\ttraining's f1: 0.296702\n",
      "[603]\ttraining's multi_logloss: 0.603694\ttraining's f1: 0.296702\n",
      "[604]\ttraining's multi_logloss: 0.603694\ttraining's f1: 0.296702\n",
      "[605]\ttraining's multi_logloss: 0.603694\ttraining's f1: 0.296702\n",
      "[606]\ttraining's multi_logloss: 0.603694\ttraining's f1: 0.296702\n",
      "[607]\ttraining's multi_logloss: 0.603694\ttraining's f1: 0.296702\n",
      "[608]\ttraining's multi_logloss: 0.603694\ttraining's f1: 0.296702\n",
      "[609]\ttraining's multi_logloss: 0.603694\ttraining's f1: 0.296702\n",
      "[610]\ttraining's multi_logloss: 0.603694\ttraining's f1: 0.296702\n",
      "[611]\ttraining's multi_logloss: 0.603694\ttraining's f1: 0.296702\n",
      "[612]\ttraining's multi_logloss: 0.603694\ttraining's f1: 0.296702\n",
      "[613]\ttraining's multi_logloss: 0.603693\ttraining's f1: 0.296702\n",
      "[614]\ttraining's multi_logloss: 0.603693\ttraining's f1: 0.296702\n",
      "[615]\ttraining's multi_logloss: 0.603693\ttraining's f1: 0.296702\n",
      "[616]\ttraining's multi_logloss: 0.603693\ttraining's f1: 0.296702\n",
      "[617]\ttraining's multi_logloss: 0.603693\ttraining's f1: 0.296702\n",
      "[618]\ttraining's multi_logloss: 0.603693\ttraining's f1: 0.296702\n",
      "[619]\ttraining's multi_logloss: 0.603693\ttraining's f1: 0.296702\n",
      "[620]\ttraining's multi_logloss: 0.603693\ttraining's f1: 0.296702\n",
      "[621]\ttraining's multi_logloss: 0.603693\ttraining's f1: 0.296702\n",
      "[622]\ttraining's multi_logloss: 0.603693\ttraining's f1: 0.296702\n",
      "[623]\ttraining's multi_logloss: 0.603693\ttraining's f1: 0.296702\n",
      "[624]\ttraining's multi_logloss: 0.603692\ttraining's f1: 0.296702\n",
      "[625]\ttraining's multi_logloss: 0.603692\ttraining's f1: 0.296702\n",
      "[626]\ttraining's multi_logloss: 0.603692\ttraining's f1: 0.296702\n",
      "[627]\ttraining's multi_logloss: 0.603692\ttraining's f1: 0.296702\n",
      "[628]\ttraining's multi_logloss: 0.603692\ttraining's f1: 0.296702\n",
      "[629]\ttraining's multi_logloss: 0.603692\ttraining's f1: 0.296702\n",
      "[630]\ttraining's multi_logloss: 0.603692\ttraining's f1: 0.296702\n",
      "[631]\ttraining's multi_logloss: 0.603692\ttraining's f1: 0.296702\n",
      "[632]\ttraining's multi_logloss: 0.603692\ttraining's f1: 0.296702\n",
      "[633]\ttraining's multi_logloss: 0.603692\ttraining's f1: 0.296702\n",
      "[634]\ttraining's multi_logloss: 0.603692\ttraining's f1: 0.296702\n",
      "[635]\ttraining's multi_logloss: 0.603692\ttraining's f1: 0.296702\n",
      "[636]\ttraining's multi_logloss: 0.603692\ttraining's f1: 0.296702\n",
      "[637]\ttraining's multi_logloss: 0.603691\ttraining's f1: 0.296702\n",
      "[638]\ttraining's multi_logloss: 0.603691\ttraining's f1: 0.296702\n",
      "[639]\ttraining's multi_logloss: 0.603691\ttraining's f1: 0.296702\n",
      "[640]\ttraining's multi_logloss: 0.603691\ttraining's f1: 0.296702\n",
      "[641]\ttraining's multi_logloss: 0.603691\ttraining's f1: 0.296702\n",
      "[642]\ttraining's multi_logloss: 0.603691\ttraining's f1: 0.296702\n",
      "[643]\ttraining's multi_logloss: 0.603691\ttraining's f1: 0.296702\n",
      "[644]\ttraining's multi_logloss: 0.603691\ttraining's f1: 0.296702\n",
      "[645]\ttraining's multi_logloss: 0.603691\ttraining's f1: 0.296702\n",
      "[646]\ttraining's multi_logloss: 0.603691\ttraining's f1: 0.296702\n",
      "[647]\ttraining's multi_logloss: 0.603691\ttraining's f1: 0.296702\n",
      "[648]\ttraining's multi_logloss: 0.60369\ttraining's f1: 0.296702\n",
      "[649]\ttraining's multi_logloss: 0.60369\ttraining's f1: 0.296702\n",
      "[650]\ttraining's multi_logloss: 0.60369\ttraining's f1: 0.296702\n",
      "[651]\ttraining's multi_logloss: 0.60369\ttraining's f1: 0.296702\n",
      "[652]\ttraining's multi_logloss: 0.60369\ttraining's f1: 0.296702\n",
      "[653]\ttraining's multi_logloss: 0.60369\ttraining's f1: 0.296702\n",
      "[654]\ttraining's multi_logloss: 0.60369\ttraining's f1: 0.296702\n",
      "[655]\ttraining's multi_logloss: 0.60369\ttraining's f1: 0.296702\n",
      "[656]\ttraining's multi_logloss: 0.60369\ttraining's f1: 0.296702\n",
      "[657]\ttraining's multi_logloss: 0.60369\ttraining's f1: 0.296702\n",
      "[658]\ttraining's multi_logloss: 0.60369\ttraining's f1: 0.296702\n",
      "[659]\ttraining's multi_logloss: 0.60369\ttraining's f1: 0.296702\n",
      "[660]\ttraining's multi_logloss: 0.603689\ttraining's f1: 0.296702\n",
      "[661]\ttraining's multi_logloss: 0.603689\ttraining's f1: 0.296702\n",
      "[662]\ttraining's multi_logloss: 0.603689\ttraining's f1: 0.296702\n",
      "[663]\ttraining's multi_logloss: 0.603689\ttraining's f1: 0.296702\n",
      "[664]\ttraining's multi_logloss: 0.603689\ttraining's f1: 0.296702\n",
      "[665]\ttraining's multi_logloss: 0.603689\ttraining's f1: 0.296702\n",
      "[666]\ttraining's multi_logloss: 0.603689\ttraining's f1: 0.296702\n",
      "[667]\ttraining's multi_logloss: 0.603689\ttraining's f1: 0.296702\n",
      "[668]\ttraining's multi_logloss: 0.603689\ttraining's f1: 0.296702\n",
      "[669]\ttraining's multi_logloss: 0.603689\ttraining's f1: 0.296702\n",
      "[670]\ttraining's multi_logloss: 0.603689\ttraining's f1: 0.296702\n",
      "[671]\ttraining's multi_logloss: 0.603689\ttraining's f1: 0.296702\n",
      "[672]\ttraining's multi_logloss: 0.603689\ttraining's f1: 0.296702\n",
      "[673]\ttraining's multi_logloss: 0.603688\ttraining's f1: 0.2981\n",
      "[674]\ttraining's multi_logloss: 0.603688\ttraining's f1: 0.296702\n",
      "[675]\ttraining's multi_logloss: 0.603688\ttraining's f1: 0.296702\n",
      "[676]\ttraining's multi_logloss: 0.603688\ttraining's f1: 0.296702\n",
      "[677]\ttraining's multi_logloss: 0.603688\ttraining's f1: 0.296702\n",
      "[678]\ttraining's multi_logloss: 0.603688\ttraining's f1: 0.296702\n",
      "[679]\ttraining's multi_logloss: 0.603688\ttraining's f1: 0.296702\n",
      "[680]\ttraining's multi_logloss: 0.603688\ttraining's f1: 0.296702\n",
      "[681]\ttraining's multi_logloss: 0.603688\ttraining's f1: 0.296702\n",
      "[682]\ttraining's multi_logloss: 0.603688\ttraining's f1: 0.296702\n",
      "[683]\ttraining's multi_logloss: 0.603688\ttraining's f1: 0.296702\n",
      "[684]\ttraining's multi_logloss: 0.603688\ttraining's f1: 0.2981\n",
      "[685]\ttraining's multi_logloss: 0.603688\ttraining's f1: 0.2981\n",
      "[686]\ttraining's multi_logloss: 0.603688\ttraining's f1: 0.2981\n",
      "[687]\ttraining's multi_logloss: 0.603687\ttraining's f1: 0.2981\n",
      "[688]\ttraining's multi_logloss: 0.603687\ttraining's f1: 0.2981\n",
      "[689]\ttraining's multi_logloss: 0.603687\ttraining's f1: 0.2981\n",
      "[690]\ttraining's multi_logloss: 0.603687\ttraining's f1: 0.2981\n",
      "[691]\ttraining's multi_logloss: 0.603687\ttraining's f1: 0.2981\n",
      "[692]\ttraining's multi_logloss: 0.603687\ttraining's f1: 0.2981\n",
      "[693]\ttraining's multi_logloss: 0.603687\ttraining's f1: 0.2981\n",
      "[694]\ttraining's multi_logloss: 0.603687\ttraining's f1: 0.2981\n",
      "[695]\ttraining's multi_logloss: 0.603687\ttraining's f1: 0.2981\n",
      "[696]\ttraining's multi_logloss: 0.603687\ttraining's f1: 0.2981\n",
      "[697]\ttraining's multi_logloss: 0.603687\ttraining's f1: 0.2981\n",
      "[698]\ttraining's multi_logloss: 0.603687\ttraining's f1: 0.2981\n",
      "[699]\ttraining's multi_logloss: 0.603687\ttraining's f1: 0.2981\n",
      "[700]\ttraining's multi_logloss: 0.603687\ttraining's f1: 0.2981\n",
      "[701]\ttraining's multi_logloss: 0.603686\ttraining's f1: 0.2981\n",
      "[702]\ttraining's multi_logloss: 0.603686\ttraining's f1: 0.2981\n",
      "[703]\ttraining's multi_logloss: 0.603686\ttraining's f1: 0.2981\n",
      "[704]\ttraining's multi_logloss: 0.603686\ttraining's f1: 0.296702\n",
      "[705]\ttraining's multi_logloss: 0.603686\ttraining's f1: 0.296702\n",
      "[706]\ttraining's multi_logloss: 0.603686\ttraining's f1: 0.296702\n",
      "[707]\ttraining's multi_logloss: 0.603686\ttraining's f1: 0.296702\n",
      "[708]\ttraining's multi_logloss: 0.603686\ttraining's f1: 0.296702\n",
      "[709]\ttraining's multi_logloss: 0.603686\ttraining's f1: 0.296702\n",
      "[710]\ttraining's multi_logloss: 0.603686\ttraining's f1: 0.296702\n",
      "[711]\ttraining's multi_logloss: 0.603686\ttraining's f1: 0.296702\n",
      "[712]\ttraining's multi_logloss: 0.603686\ttraining's f1: 0.296702\n",
      "[713]\ttraining's multi_logloss: 0.603686\ttraining's f1: 0.296702\n",
      "[714]\ttraining's multi_logloss: 0.603686\ttraining's f1: 0.296702\n",
      "[715]\ttraining's multi_logloss: 0.603685\ttraining's f1: 0.296702\n",
      "[716]\ttraining's multi_logloss: 0.603685\ttraining's f1: 0.296702\n",
      "[717]\ttraining's multi_logloss: 0.603685\ttraining's f1: 0.296702\n",
      "[718]\ttraining's multi_logloss: 0.603685\ttraining's f1: 0.296702\n",
      "[719]\ttraining's multi_logloss: 0.603685\ttraining's f1: 0.296702\n",
      "[720]\ttraining's multi_logloss: 0.603685\ttraining's f1: 0.296702\n",
      "[721]\ttraining's multi_logloss: 0.603685\ttraining's f1: 0.296702\n",
      "[722]\ttraining's multi_logloss: 0.603685\ttraining's f1: 0.296702\n",
      "[723]\ttraining's multi_logloss: 0.603685\ttraining's f1: 0.2981\n",
      "[724]\ttraining's multi_logloss: 0.603685\ttraining's f1: 0.2981\n",
      "[725]\ttraining's multi_logloss: 0.603685\ttraining's f1: 0.2981\n",
      "[726]\ttraining's multi_logloss: 0.603685\ttraining's f1: 0.2981\n",
      "[727]\ttraining's multi_logloss: 0.603685\ttraining's f1: 0.296702\n",
      "[728]\ttraining's multi_logloss: 0.603685\ttraining's f1: 0.296702\n",
      "[729]\ttraining's multi_logloss: 0.603685\ttraining's f1: 0.2981\n",
      "[730]\ttraining's multi_logloss: 0.603684\ttraining's f1: 0.2981\n",
      "[731]\ttraining's multi_logloss: 0.603684\ttraining's f1: 0.2981\n",
      "[732]\ttraining's multi_logloss: 0.603684\ttraining's f1: 0.296702\n",
      "[733]\ttraining's multi_logloss: 0.603684\ttraining's f1: 0.296702\n",
      "[734]\ttraining's multi_logloss: 0.603684\ttraining's f1: 0.296702\n",
      "[735]\ttraining's multi_logloss: 0.603684\ttraining's f1: 0.296702\n",
      "[736]\ttraining's multi_logloss: 0.603684\ttraining's f1: 0.296702\n",
      "[737]\ttraining's multi_logloss: 0.603684\ttraining's f1: 0.296702\n",
      "[738]\ttraining's multi_logloss: 0.603684\ttraining's f1: 0.2981\n",
      "[739]\ttraining's multi_logloss: 0.603684\ttraining's f1: 0.2981\n",
      "[740]\ttraining's multi_logloss: 0.603684\ttraining's f1: 0.2981\n",
      "[741]\ttraining's multi_logloss: 0.603684\ttraining's f1: 0.2981\n",
      "[742]\ttraining's multi_logloss: 0.603684\ttraining's f1: 0.2981\n",
      "[743]\ttraining's multi_logloss: 0.603684\ttraining's f1: 0.2981\n",
      "[744]\ttraining's multi_logloss: 0.603684\ttraining's f1: 0.2981\n",
      "[745]\ttraining's multi_logloss: 0.603683\ttraining's f1: 0.2981\n",
      "[746]\ttraining's multi_logloss: 0.603683\ttraining's f1: 0.2981\n",
      "[747]\ttraining's multi_logloss: 0.603683\ttraining's f1: 0.2981\n",
      "[748]\ttraining's multi_logloss: 0.603683\ttraining's f1: 0.2981\n",
      "[749]\ttraining's multi_logloss: 0.603683\ttraining's f1: 0.296702\n",
      "[750]\ttraining's multi_logloss: 0.603683\ttraining's f1: 0.2981\n",
      "[751]\ttraining's multi_logloss: 0.603683\ttraining's f1: 0.2981\n",
      "[752]\ttraining's multi_logloss: 0.603683\ttraining's f1: 0.2981\n",
      "[753]\ttraining's multi_logloss: 0.603683\ttraining's f1: 0.2981\n",
      "[754]\ttraining's multi_logloss: 0.603683\ttraining's f1: 0.2981\n",
      "[755]\ttraining's multi_logloss: 0.603683\ttraining's f1: 0.2981\n",
      "[756]\ttraining's multi_logloss: 0.603683\ttraining's f1: 0.296702\n",
      "[757]\ttraining's multi_logloss: 0.603683\ttraining's f1: 0.296702\n",
      "[758]\ttraining's multi_logloss: 0.603683\ttraining's f1: 0.2981\n",
      "[759]\ttraining's multi_logloss: 0.603683\ttraining's f1: 0.2981\n",
      "[760]\ttraining's multi_logloss: 0.603683\ttraining's f1: 0.2981\n",
      "[761]\ttraining's multi_logloss: 0.603683\ttraining's f1: 0.2981\n",
      "[762]\ttraining's multi_logloss: 0.603683\ttraining's f1: 0.2981\n",
      "[763]\ttraining's multi_logloss: 0.603682\ttraining's f1: 0.296702\n",
      "[764]\ttraining's multi_logloss: 0.603682\ttraining's f1: 0.296702\n",
      "[765]\ttraining's multi_logloss: 0.603682\ttraining's f1: 0.296702\n",
      "[766]\ttraining's multi_logloss: 0.603682\ttraining's f1: 0.2981\n",
      "[767]\ttraining's multi_logloss: 0.603682\ttraining's f1: 0.2981\n",
      "[768]\ttraining's multi_logloss: 0.603682\ttraining's f1: 0.296702\n",
      "[769]\ttraining's multi_logloss: 0.603682\ttraining's f1: 0.296702\n",
      "[770]\ttraining's multi_logloss: 0.603682\ttraining's f1: 0.296702\n",
      "[771]\ttraining's multi_logloss: 0.603682\ttraining's f1: 0.296702\n",
      "[772]\ttraining's multi_logloss: 0.603682\ttraining's f1: 0.296702\n",
      "[773]\ttraining's multi_logloss: 0.603682\ttraining's f1: 0.296702\n",
      "[774]\ttraining's multi_logloss: 0.603682\ttraining's f1: 0.296702\n",
      "[775]\ttraining's multi_logloss: 0.603682\ttraining's f1: 0.296702\n",
      "[776]\ttraining's multi_logloss: 0.603682\ttraining's f1: 0.296702\n",
      "[777]\ttraining's multi_logloss: 0.603682\ttraining's f1: 0.296702\n",
      "[778]\ttraining's multi_logloss: 0.603682\ttraining's f1: 0.296702\n",
      "[779]\ttraining's multi_logloss: 0.603682\ttraining's f1: 0.296702\n",
      "[780]\ttraining's multi_logloss: 0.603682\ttraining's f1: 0.296237\n",
      "[781]\ttraining's multi_logloss: 0.603681\ttraining's f1: 0.296702\n",
      "[782]\ttraining's multi_logloss: 0.603681\ttraining's f1: 0.296237\n",
      "[783]\ttraining's multi_logloss: 0.603681\ttraining's f1: 0.296237\n",
      "[784]\ttraining's multi_logloss: 0.603681\ttraining's f1: 0.296237\n",
      "[785]\ttraining's multi_logloss: 0.603681\ttraining's f1: 0.296237\n",
      "[786]\ttraining's multi_logloss: 0.603681\ttraining's f1: 0.296237\n",
      "[787]\ttraining's multi_logloss: 0.603681\ttraining's f1: 0.296702\n",
      "[788]\ttraining's multi_logloss: 0.603681\ttraining's f1: 0.296702\n",
      "[789]\ttraining's multi_logloss: 0.603681\ttraining's f1: 0.296237\n",
      "[790]\ttraining's multi_logloss: 0.603681\ttraining's f1: 0.296702\n",
      "[791]\ttraining's multi_logloss: 0.603681\ttraining's f1: 0.296237\n",
      "[792]\ttraining's multi_logloss: 0.603681\ttraining's f1: 0.296702\n",
      "[793]\ttraining's multi_logloss: 0.603681\ttraining's f1: 0.296702\n",
      "[794]\ttraining's multi_logloss: 0.603681\ttraining's f1: 0.296702\n",
      "[795]\ttraining's multi_logloss: 0.603681\ttraining's f1: 0.296702\n",
      "[796]\ttraining's multi_logloss: 0.603681\ttraining's f1: 0.296702\n",
      "[797]\ttraining's multi_logloss: 0.603681\ttraining's f1: 0.296702\n",
      "[798]\ttraining's multi_logloss: 0.603681\ttraining's f1: 0.296702\n",
      "[799]\ttraining's multi_logloss: 0.60368\ttraining's f1: 0.296702\n",
      "[800]\ttraining's multi_logloss: 0.60368\ttraining's f1: 0.296702\n",
      "[801]\ttraining's multi_logloss: 0.60368\ttraining's f1: 0.296702\n",
      "[802]\ttraining's multi_logloss: 0.60368\ttraining's f1: 0.296702\n",
      "[803]\ttraining's multi_logloss: 0.60368\ttraining's f1: 0.296702\n",
      "[804]\ttraining's multi_logloss: 0.60368\ttraining's f1: 0.296702\n",
      "[805]\ttraining's multi_logloss: 0.60368\ttraining's f1: 0.296702\n",
      "[806]\ttraining's multi_logloss: 0.60368\ttraining's f1: 0.296702\n",
      "[807]\ttraining's multi_logloss: 0.60368\ttraining's f1: 0.296702\n",
      "[808]\ttraining's multi_logloss: 0.60368\ttraining's f1: 0.296702\n",
      "[809]\ttraining's multi_logloss: 0.60368\ttraining's f1: 0.296702\n",
      "[810]\ttraining's multi_logloss: 0.60368\ttraining's f1: 0.296702\n",
      "[811]\ttraining's multi_logloss: 0.60368\ttraining's f1: 0.296702\n",
      "[812]\ttraining's multi_logloss: 0.60368\ttraining's f1: 0.296702\n",
      "[813]\ttraining's multi_logloss: 0.60368\ttraining's f1: 0.296702\n",
      "[814]\ttraining's multi_logloss: 0.60368\ttraining's f1: 0.296702\n",
      "[815]\ttraining's multi_logloss: 0.60368\ttraining's f1: 0.296702\n",
      "[816]\ttraining's multi_logloss: 0.60368\ttraining's f1: 0.296702\n",
      "[817]\ttraining's multi_logloss: 0.603679\ttraining's f1: 0.296702\n",
      "[818]\ttraining's multi_logloss: 0.603679\ttraining's f1: 0.296702\n",
      "[819]\ttraining's multi_logloss: 0.603679\ttraining's f1: 0.296702\n",
      "[820]\ttraining's multi_logloss: 0.603679\ttraining's f1: 0.296702\n",
      "[821]\ttraining's multi_logloss: 0.603679\ttraining's f1: 0.296702\n",
      "[822]\ttraining's multi_logloss: 0.603679\ttraining's f1: 0.296702\n",
      "[823]\ttraining's multi_logloss: 0.603679\ttraining's f1: 0.296702\n",
      "[824]\ttraining's multi_logloss: 0.603679\ttraining's f1: 0.296702\n",
      "[825]\ttraining's multi_logloss: 0.603679\ttraining's f1: 0.296702\n",
      "[826]\ttraining's multi_logloss: 0.603679\ttraining's f1: 0.296702\n",
      "[827]\ttraining's multi_logloss: 0.603679\ttraining's f1: 0.296702\n",
      "[828]\ttraining's multi_logloss: 0.603679\ttraining's f1: 0.296702\n",
      "[829]\ttraining's multi_logloss: 0.603679\ttraining's f1: 0.296702\n",
      "[830]\ttraining's multi_logloss: 0.603679\ttraining's f1: 0.296702\n",
      "[831]\ttraining's multi_logloss: 0.603679\ttraining's f1: 0.296702\n",
      "[832]\ttraining's multi_logloss: 0.603679\ttraining's f1: 0.296702\n",
      "[833]\ttraining's multi_logloss: 0.603679\ttraining's f1: 0.296702\n",
      "[834]\ttraining's multi_logloss: 0.603679\ttraining's f1: 0.296702\n",
      "[835]\ttraining's multi_logloss: 0.603679\ttraining's f1: 0.296702\n",
      "[836]\ttraining's multi_logloss: 0.603678\ttraining's f1: 0.296702\n",
      "[837]\ttraining's multi_logloss: 0.603678\ttraining's f1: 0.296702\n",
      "[838]\ttraining's multi_logloss: 0.603678\ttraining's f1: 0.296702\n",
      "[839]\ttraining's multi_logloss: 0.603678\ttraining's f1: 0.296702\n",
      "[840]\ttraining's multi_logloss: 0.603678\ttraining's f1: 0.296702\n",
      "[841]\ttraining's multi_logloss: 0.603678\ttraining's f1: 0.296702\n",
      "[842]\ttraining's multi_logloss: 0.603678\ttraining's f1: 0.296702\n",
      "[843]\ttraining's multi_logloss: 0.603678\ttraining's f1: 0.296702\n",
      "[844]\ttraining's multi_logloss: 0.603678\ttraining's f1: 0.296702\n",
      "[845]\ttraining's multi_logloss: 0.603678\ttraining's f1: 0.296702\n",
      "[846]\ttraining's multi_logloss: 0.603678\ttraining's f1: 0.296702\n",
      "[847]\ttraining's multi_logloss: 0.603678\ttraining's f1: 0.296702\n",
      "[848]\ttraining's multi_logloss: 0.603678\ttraining's f1: 0.296702\n",
      "[849]\ttraining's multi_logloss: 0.603678\ttraining's f1: 0.296702\n",
      "[850]\ttraining's multi_logloss: 0.603678\ttraining's f1: 0.296702\n",
      "[851]\ttraining's multi_logloss: 0.603678\ttraining's f1: 0.296702\n",
      "[852]\ttraining's multi_logloss: 0.603678\ttraining's f1: 0.296702\n",
      "[853]\ttraining's multi_logloss: 0.603678\ttraining's f1: 0.296702\n",
      "[854]\ttraining's multi_logloss: 0.603678\ttraining's f1: 0.296702\n",
      "[855]\ttraining's multi_logloss: 0.603678\ttraining's f1: 0.296702\n",
      "[856]\ttraining's multi_logloss: 0.603678\ttraining's f1: 0.296702\n",
      "[857]\ttraining's multi_logloss: 0.603677\ttraining's f1: 0.296702\n",
      "[858]\ttraining's multi_logloss: 0.603677\ttraining's f1: 0.296702\n",
      "[859]\ttraining's multi_logloss: 0.603677\ttraining's f1: 0.296702\n",
      "[860]\ttraining's multi_logloss: 0.603677\ttraining's f1: 0.296702\n",
      "[861]\ttraining's multi_logloss: 0.603677\ttraining's f1: 0.296702\n",
      "[862]\ttraining's multi_logloss: 0.603677\ttraining's f1: 0.296702\n",
      "[863]\ttraining's multi_logloss: 0.603677\ttraining's f1: 0.296702\n",
      "[864]\ttraining's multi_logloss: 0.603677\ttraining's f1: 0.296702\n",
      "[865]\ttraining's multi_logloss: 0.603677\ttraining's f1: 0.296702\n",
      "[866]\ttraining's multi_logloss: 0.603677\ttraining's f1: 0.296702\n",
      "[867]\ttraining's multi_logloss: 0.603677\ttraining's f1: 0.296702\n",
      "[868]\ttraining's multi_logloss: 0.603677\ttraining's f1: 0.296702\n",
      "[869]\ttraining's multi_logloss: 0.603677\ttraining's f1: 0.296702\n",
      "[870]\ttraining's multi_logloss: 0.603677\ttraining's f1: 0.296702\n",
      "[871]\ttraining's multi_logloss: 0.603677\ttraining's f1: 0.296702\n",
      "[872]\ttraining's multi_logloss: 0.603677\ttraining's f1: 0.296702\n",
      "[873]\ttraining's multi_logloss: 0.603677\ttraining's f1: 0.296702\n",
      "[874]\ttraining's multi_logloss: 0.603677\ttraining's f1: 0.296702\n",
      "[875]\ttraining's multi_logloss: 0.603677\ttraining's f1: 0.296702\n",
      "[876]\ttraining's multi_logloss: 0.603677\ttraining's f1: 0.296702\n",
      "[877]\ttraining's multi_logloss: 0.603677\ttraining's f1: 0.296702\n",
      "[878]\ttraining's multi_logloss: 0.603677\ttraining's f1: 0.296702\n",
      "[879]\ttraining's multi_logloss: 0.603676\ttraining's f1: 0.296702\n",
      "[880]\ttraining's multi_logloss: 0.603676\ttraining's f1: 0.296702\n",
      "[881]\ttraining's multi_logloss: 0.603676\ttraining's f1: 0.296702\n",
      "[882]\ttraining's multi_logloss: 0.603676\ttraining's f1: 0.296702\n",
      "[883]\ttraining's multi_logloss: 0.603676\ttraining's f1: 0.296702\n",
      "[884]\ttraining's multi_logloss: 0.603676\ttraining's f1: 0.296702\n",
      "[885]\ttraining's multi_logloss: 0.603676\ttraining's f1: 0.296702\n",
      "[886]\ttraining's multi_logloss: 0.603676\ttraining's f1: 0.296702\n",
      "[887]\ttraining's multi_logloss: 0.603676\ttraining's f1: 0.296702\n",
      "[888]\ttraining's multi_logloss: 0.603676\ttraining's f1: 0.296702\n",
      "[889]\ttraining's multi_logloss: 0.603676\ttraining's f1: 0.296702\n",
      "[890]\ttraining's multi_logloss: 0.603676\ttraining's f1: 0.296702\n",
      "[891]\ttraining's multi_logloss: 0.603676\ttraining's f1: 0.296702\n",
      "[892]\ttraining's multi_logloss: 0.603676\ttraining's f1: 0.296702\n",
      "[893]\ttraining's multi_logloss: 0.603676\ttraining's f1: 0.296702\n",
      "[894]\ttraining's multi_logloss: 0.603676\ttraining's f1: 0.296702\n",
      "[895]\ttraining's multi_logloss: 0.603676\ttraining's f1: 0.296702\n",
      "[896]\ttraining's multi_logloss: 0.603676\ttraining's f1: 0.296702\n",
      "[897]\ttraining's multi_logloss: 0.603676\ttraining's f1: 0.296702\n",
      "[898]\ttraining's multi_logloss: 0.603676\ttraining's f1: 0.296702\n",
      "[899]\ttraining's multi_logloss: 0.603676\ttraining's f1: 0.296702\n",
      "[900]\ttraining's multi_logloss: 0.603676\ttraining's f1: 0.296702\n",
      "[901]\ttraining's multi_logloss: 0.603675\ttraining's f1: 0.296702\n",
      "[902]\ttraining's multi_logloss: 0.603675\ttraining's f1: 0.296702\n",
      "[903]\ttraining's multi_logloss: 0.603675\ttraining's f1: 0.296702\n",
      "[904]\ttraining's multi_logloss: 0.603675\ttraining's f1: 0.296702\n",
      "[905]\ttraining's multi_logloss: 0.603675\ttraining's f1: 0.296702\n",
      "[906]\ttraining's multi_logloss: 0.603675\ttraining's f1: 0.296702\n",
      "[907]\ttraining's multi_logloss: 0.603675\ttraining's f1: 0.296702\n",
      "[908]\ttraining's multi_logloss: 0.603675\ttraining's f1: 0.296702\n",
      "[909]\ttraining's multi_logloss: 0.603675\ttraining's f1: 0.296702\n",
      "[910]\ttraining's multi_logloss: 0.603675\ttraining's f1: 0.296702\n",
      "[911]\ttraining's multi_logloss: 0.603675\ttraining's f1: 0.296702\n",
      "[912]\ttraining's multi_logloss: 0.603675\ttraining's f1: 0.296702\n",
      "[913]\ttraining's multi_logloss: 0.603675\ttraining's f1: 0.296702\n",
      "[914]\ttraining's multi_logloss: 0.603675\ttraining's f1: 0.296702\n",
      "[915]\ttraining's multi_logloss: 0.603675\ttraining's f1: 0.296702\n",
      "[916]\ttraining's multi_logloss: 0.603675\ttraining's f1: 0.296702\n",
      "[917]\ttraining's multi_logloss: 0.603675\ttraining's f1: 0.296702\n",
      "[918]\ttraining's multi_logloss: 0.603675\ttraining's f1: 0.296702\n",
      "[919]\ttraining's multi_logloss: 0.603675\ttraining's f1: 0.296702\n",
      "[920]\ttraining's multi_logloss: 0.603675\ttraining's f1: 0.296702\n",
      "[921]\ttraining's multi_logloss: 0.603675\ttraining's f1: 0.296702\n",
      "[922]\ttraining's multi_logloss: 0.603675\ttraining's f1: 0.296702\n",
      "[923]\ttraining's multi_logloss: 0.603675\ttraining's f1: 0.296702\n",
      "[924]\ttraining's multi_logloss: 0.603674\ttraining's f1: 0.296702\n",
      "[925]\ttraining's multi_logloss: 0.603674\ttraining's f1: 0.296702\n",
      "[926]\ttraining's multi_logloss: 0.603674\ttraining's f1: 0.296702\n",
      "[927]\ttraining's multi_logloss: 0.603674\ttraining's f1: 0.296702\n",
      "[928]\ttraining's multi_logloss: 0.603674\ttraining's f1: 0.296702\n",
      "[929]\ttraining's multi_logloss: 0.603674\ttraining's f1: 0.296702\n",
      "[930]\ttraining's multi_logloss: 0.603674\ttraining's f1: 0.296702\n",
      "[931]\ttraining's multi_logloss: 0.603674\ttraining's f1: 0.296702\n",
      "[932]\ttraining's multi_logloss: 0.603674\ttraining's f1: 0.296702\n",
      "[933]\ttraining's multi_logloss: 0.603674\ttraining's f1: 0.296702\n",
      "[934]\ttraining's multi_logloss: 0.603674\ttraining's f1: 0.296702\n",
      "[935]\ttraining's multi_logloss: 0.603674\ttraining's f1: 0.296702\n",
      "[936]\ttraining's multi_logloss: 0.603674\ttraining's f1: 0.296702\n",
      "[937]\ttraining's multi_logloss: 0.603674\ttraining's f1: 0.296702\n",
      "[938]\ttraining's multi_logloss: 0.603674\ttraining's f1: 0.296702\n",
      "[939]\ttraining's multi_logloss: 0.603674\ttraining's f1: 0.296702\n",
      "[940]\ttraining's multi_logloss: 0.603674\ttraining's f1: 0.296702\n",
      "[941]\ttraining's multi_logloss: 0.603674\ttraining's f1: 0.296702\n",
      "[942]\ttraining's multi_logloss: 0.603674\ttraining's f1: 0.296702\n",
      "[943]\ttraining's multi_logloss: 0.603674\ttraining's f1: 0.296702\n",
      "[944]\ttraining's multi_logloss: 0.603674\ttraining's f1: 0.296702\n",
      "[945]\ttraining's multi_logloss: 0.603674\ttraining's f1: 0.296702\n",
      "[946]\ttraining's multi_logloss: 0.603674\ttraining's f1: 0.296702\n",
      "[947]\ttraining's multi_logloss: 0.603673\ttraining's f1: 0.296702\n",
      "[948]\ttraining's multi_logloss: 0.603673\ttraining's f1: 0.296702\n",
      "[949]\ttraining's multi_logloss: 0.603673\ttraining's f1: 0.296702\n",
      "[950]\ttraining's multi_logloss: 0.603673\ttraining's f1: 0.296702\n",
      "[951]\ttraining's multi_logloss: 0.603673\ttraining's f1: 0.296702\n",
      "[952]\ttraining's multi_logloss: 0.603673\ttraining's f1: 0.296702\n",
      "[953]\ttraining's multi_logloss: 0.603673\ttraining's f1: 0.296702\n",
      "[954]\ttraining's multi_logloss: 0.603673\ttraining's f1: 0.296702\n",
      "[955]\ttraining's multi_logloss: 0.603673\ttraining's f1: 0.296702\n",
      "[956]\ttraining's multi_logloss: 0.603673\ttraining's f1: 0.296702\n",
      "[957]\ttraining's multi_logloss: 0.603673\ttraining's f1: 0.296702\n",
      "[958]\ttraining's multi_logloss: 0.603673\ttraining's f1: 0.296702\n",
      "[959]\ttraining's multi_logloss: 0.603673\ttraining's f1: 0.296702\n",
      "[960]\ttraining's multi_logloss: 0.603673\ttraining's f1: 0.296702\n",
      "[961]\ttraining's multi_logloss: 0.603673\ttraining's f1: 0.296702\n",
      "[962]\ttraining's multi_logloss: 0.603673\ttraining's f1: 0.296702\n",
      "[963]\ttraining's multi_logloss: 0.603673\ttraining's f1: 0.296702\n",
      "[964]\ttraining's multi_logloss: 0.603673\ttraining's f1: 0.296702\n",
      "[965]\ttraining's multi_logloss: 0.603673\ttraining's f1: 0.296702\n",
      "[966]\ttraining's multi_logloss: 0.603673\ttraining's f1: 0.296702\n",
      "[967]\ttraining's multi_logloss: 0.603673\ttraining's f1: 0.296702\n",
      "[968]\ttraining's multi_logloss: 0.603673\ttraining's f1: 0.296702\n",
      "[969]\ttraining's multi_logloss: 0.603673\ttraining's f1: 0.296702\n",
      "[970]\ttraining's multi_logloss: 0.603673\ttraining's f1: 0.296702\n",
      "[971]\ttraining's multi_logloss: 0.603673\ttraining's f1: 0.296702\n",
      "[972]\ttraining's multi_logloss: 0.603673\ttraining's f1: 0.296702\n",
      "[973]\ttraining's multi_logloss: 0.603672\ttraining's f1: 0.296702\n",
      "[974]\ttraining's multi_logloss: 0.603672\ttraining's f1: 0.296702\n",
      "[975]\ttraining's multi_logloss: 0.603672\ttraining's f1: 0.296702\n",
      "[976]\ttraining's multi_logloss: 0.603672\ttraining's f1: 0.296702\n",
      "[977]\ttraining's multi_logloss: 0.603672\ttraining's f1: 0.296702\n",
      "[978]\ttraining's multi_logloss: 0.603672\ttraining's f1: 0.296237\n",
      "[979]\ttraining's multi_logloss: 0.603672\ttraining's f1: 0.296237\n",
      "[980]\ttraining's multi_logloss: 0.603672\ttraining's f1: 0.296237\n",
      "[981]\ttraining's multi_logloss: 0.603672\ttraining's f1: 0.296702\n",
      "[982]\ttraining's multi_logloss: 0.603672\ttraining's f1: 0.296702\n",
      "[983]\ttraining's multi_logloss: 0.603672\ttraining's f1: 0.296702\n",
      "[984]\ttraining's multi_logloss: 0.603672\ttraining's f1: 0.296702\n",
      "[985]\ttraining's multi_logloss: 0.603672\ttraining's f1: 0.296702\n",
      "[986]\ttraining's multi_logloss: 0.603672\ttraining's f1: 0.296237\n",
      "[987]\ttraining's multi_logloss: 0.603672\ttraining's f1: 0.296237\n",
      "[988]\ttraining's multi_logloss: 0.603672\ttraining's f1: 0.296702\n",
      "[989]\ttraining's multi_logloss: 0.603672\ttraining's f1: 0.296237\n",
      "[990]\ttraining's multi_logloss: 0.603672\ttraining's f1: 0.296237\n",
      "[991]\ttraining's multi_logloss: 0.603672\ttraining's f1: 0.296237\n",
      "[992]\ttraining's multi_logloss: 0.603672\ttraining's f1: 0.296702\n",
      "[993]\ttraining's multi_logloss: 0.603672\ttraining's f1: 0.296702\n",
      "[994]\ttraining's multi_logloss: 0.603672\ttraining's f1: 0.296702\n",
      "[995]\ttraining's multi_logloss: 0.603672\ttraining's f1: 0.296702\n",
      "[996]\ttraining's multi_logloss: 0.603672\ttraining's f1: 0.296702\n",
      "[997]\ttraining's multi_logloss: 0.603672\ttraining's f1: 0.296702\n",
      "[998]\ttraining's multi_logloss: 0.603672\ttraining's f1: 0.296702\n",
      "[999]\ttraining's multi_logloss: 0.603672\ttraining's f1: 0.296702\n",
      "[1000]\ttraining's multi_logloss: 0.603671\ttraining's f1: 0.296237\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.astype(float)\n",
    "x_valid = x_valid.astype(float)\n",
    "lgb_train = lgb.Dataset(x_train, y_train)\n",
    "model = lgb.train(\n",
    "    {\n",
    "        \"objective\": \"multiclass\",\n",
    "        \"num_class\": 3,\n",
    "    },\n",
    "    lgb_train,\n",
    "    valid_sets=[lgb_train],\n",
    "    num_boost_round=1000,\n",
    "    feval=mean_f1score\n",
    "    # feval=mean_recall\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2937774046086389"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(x_valid)\n",
    "f1_score(y_valid, y_pred.argmax(axis=1), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3996\n",
       "2       1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_pred.argmax(axis=1)).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: >"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(model.feature_importance(), index=x_train.columns).sort_values(ascending=False).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_problem(raw_df:pd.DataFrame, problem_unique:set=None):\n",
    "    \"\"\"\n",
    "    与えられたDataFrameに対して、problemsを分割して、各問題が含まれているかどうかを特徴量として追加する\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    raw_df : pd.DataFrame\n",
    "        problemsを含むDataFrame\n",
    "    problem_unique : set\n",
    "        problemsに含まれる問題の集合(testデータを処理する場合には、trainデータの問題の集合を渡す)\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        problemsを分割して、各問題が含まれているかどうかを特徴量として追加したDataFrame\n",
    "    problem_unique : set\n",
    "        problemsに含まれる問題の集合(testデータを処理する場合にはNoneを返す)\n",
    "\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import pandas as pd\n",
    "    >>> train = pd.read_csv(\"data/official/train.csv\", index_col=0)\n",
    "    >>> test = pd.read_csv(\"data/official/test.csv\", index_col=0)\n",
    "    >>> processed_train, problem_unique = transform_problem(train)\n",
    "    >>> processed_test, _ = transform_problem(test, problem_unique)\n",
    "    \"\"\"\n",
    "    def _split_string_by_uppercase(input_string):\n",
    "        result = []\n",
    "        current_word = \"\"\n",
    "        if not type(input_string)==str:\n",
    "            return result\n",
    "\n",
    "        # Escape String \"Other\"\n",
    "        for char in input_string:\n",
    "            if char.isupper() and current_word:\n",
    "                result.append(current_word)\n",
    "                current_word = char\n",
    "            else:\n",
    "                current_word += char\n",
    "\n",
    "        if current_word:\n",
    "            result.append(current_word)\n",
    "\n",
    "        return result\n",
    "\n",
    "    df = raw_df.copy()\n",
    "    df.loc[:,\"problems_list\"] = df.problems.apply(_split_string_by_uppercase)\n",
    "    is_train = False\n",
    "    if problem_unique is None:\n",
    "        is_train = True\n",
    "        problem_unique = set()\n",
    "        for problems in df.loc[:,\"problems\"].unique():\n",
    "            for problem in _split_string_by_uppercase(problems):\n",
    "                problem_unique.add(problem)\n",
    "    for unique_problem in problem_unique:\n",
    "        df.loc[:,f\"is_problem_{unique_problem}\"] = df.loc[:,\"problems_list\"].apply(lambda x: unique_problem in x)\n",
    "        df.loc[:,f\"is_problem_{unique_problem}\"] = df.loc[:,f\"is_problem_{unique_problem}\"].fillna(False)\n",
    "    df.loc[:,\"problems_count\"] = df.loc[:,\"problems_list\"].apply(len)\n",
    "    df = df.drop(columns=[\"problems_list\"])\n",
    "    if is_train:\n",
    "        return df, problem_unique\n",
    "    else:\n",
    "        return df, None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "察しの良い読者ならもう既に気づいていると思うが、この変換はOneHotエンコーディングと大文字を数えた特徴量の生成でしかない。「問題箇所」と「問題内容」を文字列を組み合わせた特徴量生成を考えている。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
